\section{Introduction}
\label{sec:intro}

% eleborate on motivation
In recent years we have seen an astonishing success in neural field models, for compressed scene representation, as well as in autoregressive
transformer-based generative models (eg. Large Language Models).
Additionally, first approaches towards generating novel neural fields using diffusion models \cite{erko√ß2023hyperdiffusion} and unconditional triangle-mesh generation using transformers \cite{siddiqui2023meshgpt} have been proposed.
% and problem statement
However, the absence of a clear structure of implicit neural fields (NeF) makes it difficult to apply to an autoregressive process, which assumes sequence data. Another Problem is the continuous nature of MLP weights, while transformers typically use a small to medium sized vocabulary.
Therefore, the main problem lies in finding a representation of the MLP weights that can be used to train sequence-to-sequence architectures.
% contribution bullet points

\noindent \textbf{Our contributions are:}
\begin{itemize}
    \item Combine the idea of autoregressive transformer-based generation with neural fields
    \item Quantitatively compare different embedding strategies
\end{itemize}
