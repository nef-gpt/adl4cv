abstract

learn a vocabulary of latent embeddings - using some kind of neighbourhood encoding
find positional encoding based on graph structure of the neural network
experiment with ordering of the weight


IDEA:

ordering of the weights (depth-first search)



TODO:

- research attention-based learning for unordered data
- autoregressive models for continuous values / how to predict the token if token is a continuous number (eg. float)
- Intro NEF
- Intro Transformer (also maybe regression transformer)
- Intro autoregressive models for generation tasks
- Intro novel methods used (CNN Graph Neighbor Embedding (MeshGPT), Positional Encoding for MLP Weights)