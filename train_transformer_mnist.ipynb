{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import torchvision\n",
            "from vector_quantize_pytorch import VectorQuantize\n",
            "import os\n",
            "from data.nef_mnist_dataset import MnistNeFDataset, TokenTransform\n",
            "\n",
            "from networks.nano_gpt import GPTConfig\n",
            "\n",
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
            "data_root = os.path.join(dir_path, \"adl4cv\")\n",
            "\n",
            "# load used vector quantizer\n",
            "vq_dicts = torch.load(os.path.join(data_root, \"models\", \"vqs\", \"vq_mnist_with_all_5_conditioned_n_501.pt\"))\n",
            "vq = VectorQuantize(**vq_dicts[\"vq_config\"])\n",
            "vq.load_state_dict(vq_dicts[\"state_dict\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "len(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset_model = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), **kwargs)\n",
            "\n",
            "for key in dataset_model[0][0][\"state_dict\"].keys():\n",
            "  print(f'{key} -> {dataset_model[0][0][\"state_dict\"][key].shape}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "from training import training_nano_gpt\n",
            "\n",
            "\n",
            "config = training_nano_gpt.Config()\n",
            "config.learning_rate=9e-4\n",
            "config.max_iters = 30000\n",
            "config.weight_decay=0.00\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=config.max_iters\n",
            "config.warmup_iters=0.05*config.max_iters\n",
            "config.batch_size = 256\n",
            "config.gradient_accumulation_steps = 1\n",
            "config.init_from = \"scratch\"\n",
            "config.out_dir =\"models/token_transformer\"\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "config.eval_interval = 250\n",
            "config.metric_interval = 250\n",
            "\n",
            "model_config = GPTConfig(n_embd=300, block_size=128, n_head=12, n_layer=12, vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 11, dropout=0.0, max_len=len(dataset[0][0]) + 1)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "cb_size = vq_dicts[\"vq_config\"][\"codebook_size\"]\n",
            "token_dict = {\n",
            "    \"SOS\": cb_size + 0,\n",
            "    \"0\": cb_size + 10,\n",
            "    \"1\": cb_size + 9,\n",
            "    \"2\": cb_size + 8,\n",
            "    \"3\": cb_size + 7,\n",
            "    \"4\": cb_size + 6,\n",
            "    \"5\": cb_size + 5,\n",
            "    \"6\": cb_size + 4,\n",
            "    \"7\": cb_size + 3,\n",
            "    \"8\": cb_size + 2,\n",
            "    \"9\": cb_size + 1\n",
            "}\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def create_split_indices(n, train_ratio=0.9):\n",
            "    # Generate a random permutation of indices from 0 to n-1\n",
            "    shuffled_indices = torch.randperm(n)\n",
            "    # Determine the cut-off for training data\n",
            "    train_size = int(train_ratio * n)\n",
            "    # Split indices into training and validation sets\n",
            "    train_indices = shuffled_indices[:train_size]\n",
            "    val_indices = shuffled_indices[train_size:]\n",
            "    return train_indices, val_indices\n",
            "\n",
            "train_indices, val_indices = create_split_indices(len(dataset))\n",
            "\n",
            "def get_batch_lambda(config, dataset, model_config, split):\n",
            "    batch_size = config.batch_size\n",
            "    \n",
            "\n",
            "    # Select indices based on the split\n",
            "    if split == 'train':\n",
            "        # Randomly select batch_size indices from the train_indices\n",
            "        indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
            "    elif split == 'val':\n",
            "        # Randomly select batch_size indices from the val_indices\n",
            "        indices = val_indices[torch.randint(0, len(val_indices), (batch_size,))]\n",
            "    \n",
            "    \n",
            "    # Initialize lists to hold the sequences and labels\n",
            "    samples = []\n",
            "    labels = []\n",
            "\n",
            "    # Collect samples and labels\n",
            "    for idx in indices:\n",
            "        sample, label = dataset[idx]\n",
            "        start_tokens = torch.Tensor([token_dict[\"SOS\"], token_dict[str(label)]]).long()  # Start of sequence token\n",
            "        sample = torch.cat((start_tokens, sample), dim=0)\n",
            "        #start_tokens = torch.Tensor([0]).long()  # Start of sequence token\n",
            "        #sample = torch.cat((start_tokens, sample + 1), dim=0)\n",
            "        samples.append(sample)\n",
            "        labels.append(label)\n",
            "\n",
            "    # Prepare the sequences for model input\n",
            "    max_len = samples[0].size(0)\n",
            "    x = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    y = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    \n",
            "    for i, sample in enumerate(samples):\n",
            "        end_index = sample.size(0) - 1\n",
            "        x[i, :end_index] = sample[:-1]  # Exclude the last token for x\n",
            "        y[i, :end_index] = sample[1:]   # Exclude the first token for y\n",
            "\n",
            "    # Ensure x and y are the correct shape (batch_size, block_size) if needed:\n",
            "    # Here, we truncate to `block_size` if samples are longer than `block_size`.\n",
            "\n",
            "    idx = torch.randint(0, max_len - 1 - model_config.block_size, (batch_size,))\n",
            "\n",
            "    x_cutted = torch.zeros((batch_size, model_config.block_size), dtype=torch.long)\n",
            "    y_cutted = torch.zeros((batch_size,  model_config.block_size), dtype=torch.long)\n",
            "\n",
            "    for i, offset in enumerate(idx):\n",
            "        x_cutted[i, :] = x[i, offset:offset+model_config.block_size]\n",
            "        y_cutted[i, :] = y[i, offset:offset+model_config.block_size]\n",
            "\n",
            "\n",
            "    # x and y have to be\n",
            "    x_cutted = x_cutted.to(config.device)\n",
            "    y_cutted = y_cutted.to(config.device)\n",
            "\n",
            "    return x_cutted, y_cutted, idx\n",
            "\n",
            "create_get_batch = lambda config, dataset, model_config: lambda split: get_batch_lambda(config, dataset, model_config, split)\n",
            "get_batch = create_get_batch(config, dataset, model_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Prepeare model parameters and train\n",
            "import wandb\n",
            "trained_model = training_nano_gpt.train(get_batch, config, model_config, vq, vq_dicts[\"vq_config\"], token_dict=token_dict)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from networks.nano_gpt import GPT\n",
            "from utils import get_default_device\n",
            "\n",
            "model_dict = torch.load(\"./models/token_transformer/proper_tokens.pt\")\n",
            "# Configuration\n",
            "print(model_dict.keys())\n",
            "idx = 3\n",
            "\n",
            "device = get_default_device()\n",
            "model = GPT(model_dict[\"model_args\"])#model_dict\n",
            "model.to(device=device)\n",
            "model.load_state_dict(model_dict[\"model\"])\n",
            "model.eval()\n",
            "\n",
            "vq = VectorQuantize(**model_dict[\"vq_config\"])\n",
            "vq.load_state_dict(model_dict[\"vq_state_dict\"])\n",
            "vq.eval()\n",
            "\n",
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "\n",
            "\n",
            "sample = dataset[0][0]\n",
            "X, Y = get_batch('val')\n",
            "X, Y = (X[0].unsqueeze(0), Y[0].unsqueeze(0))\n",
            "pred, _ = model(X, Y)\n",
            "print(f\"Correctly predicated tokens {pred.argmax(dim=-1)==Y}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from random import randint\n",
            "from animation.animation_util import backtransform_weights, reconstruct_image\n",
            "from networks.mlp_models import MLP3D\n",
            "\n",
            "ij_len = 3\n",
            "# Plotting the tensors as heatmaps in grayscale\n",
            "plt.style.use(\"dark_background\")\n",
            "fig, axes = plt.subplots(ij_len, ij_len, figsize=(10, 10))\n",
            "\n",
            "fig.patch.set_alpha(0)\n",
            "\n",
            "for ax in axes.flat:\n",
            "    ax.patch.set_facecolor(\"none\")\n",
            "    ax.patch.set_alpha(0)\n",
            "\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "for i in range(ij_len):\n",
            "    for j in range(ij_len):\n",
            "\n",
            "        model.eval()\n",
            "        number = str(randint(0, 9))\n",
            "        novel_tokens = model.generate(torch.Tensor([[token_dict[\"SOS\"], token_dict[number]]]).long().to(device=\"cuda\"), dataset[0][0].shape[0], temperature=0.8, top_k=3)[:, 1:]\n",
            "\n",
            "        print(number)\n",
            "        novel_tokens = novel_tokens[:, 1:].unsqueeze(-1).to(\"cpu\")\n",
            "                                                                                                                         \n",
            "\n",
            "        max_similarity = 0\n",
            "        \"\"\"\n",
            "        for data in dataset:\n",
            "            similarity = (data[0].to(device)==noxmarty/resnet-tiny-mnist\n",
            "                max_similarity = similarity\n",
            "        \"\"\"\n",
            "        #print(f\"Maximum Similarity of picture (i, j) {(i, j)}: {max_similarity}\")\n",
            "\n",
            "        novel_weights= vq.get_codes_from_indices((novel_tokens))\n",
            "\n",
            "        dataset_no_transform = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), **kwargs)\n",
            "        original_dict = dataset_no_transform[0][0]\n",
            "\n",
            "        reconstructed_dict = backtransform_weights(novel_weights, original_dict[\"state_dict\"])\n",
            "\n",
            "        mlp3d = MLP3D(**original_dict[\"model_config\"])\n",
            "        mlp3d.load_state_dict(reconstructed_dict)\n",
            "        reconstructed_tensor = reconstruct_image(mlp3d)\n",
            "\n",
            "        axes[i][j].imshow(reconstructed_tensor, cmap='gray', aspect='auto')\n",
            "        axes[i][j].set_title(number)\n",
            "        axes[i][j].axis(\"off\")\n",
            "\n",
            "fig.patch.set_alpha(0)\n",
            "\n",
            "\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Create a Tensor for every possible label with all of mnist\n",
            "mnist_dataset = torchvision.datasets.MNIST(root=\"mnist-data\", train=True, download=True)\n",
            "\n",
            "mnist = {}\n",
            "\n",
            "for label in range(10):\n",
            "  mnist_tensor = torch.stack([torchvision.transforms.functional.pil_to_tensor(mnist_dataset[i][0]) / 255.0 for i in range(len(mnist_dataset)) if mnist_dataset[i][1] == label])\n",
            "  mnist.update({ label: mnist_tensor })"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from random import randint\n",
            "from einops import rearrange\n",
            "from animation.animation_util import backtransform_weights, reconstruct_image\n",
            "from networks.mlp_models import MLP3D\n",
            "from utils.metrics import pairwise_ssim_distance\n",
            "\n",
            "# GENERATE WITH CLOSEST NEIGHBORS\n",
            "\n",
            "# where 3 is \n",
            "num_neighbors = 3\n",
            "width = 1 + num_neighbors\n",
            "height = 5\n",
            "offset = 5\n",
            "# Plotting the tensors as heatmaps in grayscale\n",
            "plt.style.use(\"dark_background\")\n",
            "fig, axes = plt.subplots(height, width, figsize=(10, (width / height) * 10))\n",
            "\n",
            "fig.patch.set_alpha(0)\n",
            "fig.tight_layout()\n",
            "\n",
            "for ax in axes.flat:\n",
            "    ax.patch.set_facecolor(\"none\")\n",
            "    ax.patch.set_alpha(0)\n",
            "\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "for row in range(height):\n",
            "    label = row + offset\n",
            "\n",
            "    model.eval()\n",
            "    novel_tokens = model.generate(torch.Tensor([[token_dict[\"SOS\"], token_dict[str(label)]]]).long().to(device=\"cuda\"), dataset[0][0].shape[0], temperature=0.87345, top_k=1)[:, 1:]\n",
            "\n",
            "    print(label)\n",
            "    novel_tokens = novel_tokens[:, 1:].unsqueeze(-1).to(\"cpu\")\n",
            "                                                                                                                        \n",
            "    #print(f\"Maximum Similarity of picture (i, j) {(i, j)}: {max_similarity}\")\n",
            "\n",
            "    novel_weights= vq.get_codes_from_indices((novel_tokens))\n",
            "\n",
            "    dataset_no_transform = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), **kwargs)\n",
            "    original_dict = dataset_no_transform[0][0]\n",
            "\n",
            "    reconstructed_dict = backtransform_weights(novel_weights, original_dict[\"state_dict\"])\n",
            "\n",
            "    mlp3d = MLP3D(**original_dict[\"model_config\"])\n",
            "    mlp3d.load_state_dict(reconstructed_dict)\n",
            "    reconstructed_tensor = torch.Tensor(rearrange( reconstruct_image(mlp3d), \"... -> 1 1 ...\"))\n",
            "\n",
            "    # now find the closest neighbor regarding the ssim_distance\n",
            "    distances = pairwise_ssim_distance(reconstructed_tensor, mnist[label])\n",
            "\n",
            "    closest = torch.sort(distances).indices[:, -num_neighbors:]\n",
            "\n",
            "    for i in range(num_neighbors):\n",
            "        closest_neighbor = mnist[label][closest[:, -(i+1)].item()]\n",
            "\n",
            "        axes[row][i + 1].imshow(closest_neighbor.squeeze(0), cmap='gray', aspect='auto')\n",
            "        axes[row][i + 1].set_title(f\"Nearest Neighbor {i + 1}\")\n",
            "        axes[row][i + 1].axis(\"off\")\n",
            "\n",
            "    axes[row][0].imshow(rearrange(reconstructed_tensor, \"1 1 w h -> w h\"), cmap='gray', aspect='auto')\n",
            "    axes[row][0].set_title(f\"Novel\")\n",
            "    axes[row][0].axis(\"off\")\n",
            "\n",
            "fig.patch.set_alpha(0)\n",
            "\n",
            "\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# get the logits of the gpt model generation\n",
            "model.eval()\n",
            "logits_conditioning = model.generate_logits(torch.Tensor([[token_dict[\"SOS\"]]]).long().to(device=\"cuda\"))\n",
            "\n",
            "# do a whole autoregressive proccess and get the logits for the last token\n",
            "novel_tokens = model.generate(torch.Tensor([[token_dict[\"SOS\"]]]).long().to(device=\"cuda\"), dataset[0][0].shape[0] + 1, temperature=0.8, top_k=3)[:, :]\n",
            "logits_last = model.generate_logits(novel_tokens[:, :-1])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# barchart of the logits\n",
            "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
            "ax.bar(range(logits_last.shape[-1]), logits_last[0].detach().cpu().numpy())\n",
            "ax.set_title(\"Logits of the conditioning tokens\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# store the logits as numpy file\n",
            "import numpy as np\n",
            "np.save(\"logits_conditioning.npy\", logits_conditioning.squeeze(0).detach().cpu().numpy())\n",
            "np.save(\"logits_last.npy\", logits_last.squeeze(0).detach().cpu().numpy())"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
