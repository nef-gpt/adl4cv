{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, idx, targets=None):\n",
    "    device = idx.device\n",
    "    b, t = idx.size()\n",
    "    assert (\n",
    "        t <= self.config.block_size\n",
    "    ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "    pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "    # forward the GPT model itself\n",
    "    tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "    x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    for block in self.transformer.h:\n",
    "        x = block(x)\n",
    "    x = self.transformer.ln_f(x)\n",
    "\n",
    "    if targets is not None:\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        logits = self.lm_head(x)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "        )\n",
    "    else:\n",
    "        # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "        logits = self.lm_head(\n",
    "            x[:, [-1], :]\n",
    "        )  # note: using list [-1] to preserve the time dim\n",
    "        loss = None\n",
    "\n",
    "    return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers of nanoGPT:\n",
    "\n",
    "* B = batch size\n",
    "* T = sequence length (block_size)\n",
    "* C = embedding dimension (n_embd)\n",
    "* V = vocabulary size (vocab_size)\n",
    "\n",
    "## Word Token Embedding (wte):\n",
    "* maps token ID to dense vector representation of specific dimension (n_embd)\n",
    "* Input (B, T) -> Output (B, T, C)\n",
    "* #learned parameters = V * C\n",
    "\n",
    "## Word Positional Embedding:\n",
    "* incorperate positional information into token embedding\n",
    "* Input (T) -> Output (T, C)\n",
    "* #learned parameters = T*C\n",
    "\n",
    "## Block:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm:\n",
    "* used to normalize the feature accross the features (n_embd) for each token in the sequence (self.weight is used for scaling and self.bias is used to shift)\n",
    "* Input (B, T, C) -> Output (B, T, C)\n",
    "* #learned parameters = 2*C (if bias is used) / C (if bias is not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casual Self-Attention\n",
    "\n",
    "*  allows the model to focus on different parts of the input sequence to generate contextually relevant representations while ensuring that each position can only attend to positions before or at its own position (to maintain the causality required for autoregressive models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project input into queries, keys and values using a single linear layer that outputs a tensor shape (B, T, 3*C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that the q, k and v are reshaped to (B, T, n_head, head_dim), where head_dim = C // n_head. Next it is transposed to shape (B, n_head, T, head_dim) which allows parallel computation across multiple attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q,\n",
    "                k,\n",
    "                v,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0,\n",
    "                is_causal=True,\n",
    "            )\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two different implementations of the Scaled Dot-Product Attention. Both do the same thing: \n",
    "* Compute the attention scores using the dot product of Q and K, scaled by the square root of the head dimension. \n",
    "* Apply a causal mask to ensure that each position can only attend to previous positions (and itself), ensuring causality\n",
    "* Use softmax to convert these scores into attention probabilities: att=softmax(att)\n",
    "* Dropout is applied to the attention probabilities to prevent overfitting.\n",
    "* The attention probabilities are used to compute a weighted sum of the values (V), resulting in an output tensor of shape (B, n_head, T, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the output tensor is reshaped by concatenating the output from all heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
