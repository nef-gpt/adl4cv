{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "from vector_quantize_pytorch import VectorQuantize\n",
            "import os\n",
            "from data.nef_shapenet_dataset import ShapeNetDataset, FlattenTransform3D, TokenTransform3D, ModelTransform3DFromTokens, ModelTransform3D\n",
            "from training import training_nano_gpt\n",
            "from utils.visualization3d import visualize_model3d, model_to_mesh\n",
            "from vector_quantize_pytorch import ResidualVQ\n",
            "from utils import get_default_device\n",
            "\n",
            "from networks.nano_gpt import GPTConfig, GPT\n",
            "\n",
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import wandb\n",
            "wandb.login()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rq_dict = torch.load(\"./models/rq_search_results/shapenet_retrained_learnable_rq_model_dim_1_vocab_127_batch_size_16_threshold_ema_dead_code_0_kmean_iters_1_num_quantizers_1_use_init_False.pth\")\n",
            "rq_dict.keys()\n",
            "\n",
            "state_dict = rq_dict[\"state_dict\"]\n",
            "rq_config = rq_dict[\"rq_config\"]\n",
            "\n",
            "rvq = ResidualVQ(**rq_config)\n",
            "rvq.load_state_dict(state_dict)\n",
            "rvq.to(get_default_device())\n",
            "\n",
            "token_transform = TokenTransform3D(rvq)\n",
            "dataset = ShapeNetDataset(os.path.join(\"./\", \"datasets\", \"shapenet_nef_2\", \"pretrained\"))\n",
            "\n",
            "model_transform = ModelTransform3D(dataset[0][0][\"model_config\"])\n",
            "\n",
            "dataset_model = ShapeNetDataset(os.path.join(\"./\", \"datasets\", \"shapenet_nef_2\", \"pretrained\"), transform=model_transform)\n",
            "dataset_token = ShapeNetDataset(os.path.join(\"./\", \"datasets\", \"shapenet_nef_2\", \"pretrained\"), transform=token_transform)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "config = training_nano_gpt.Config()\n",
            "config.learning_rate=2e-3\n",
            "config.eval_interval = 25\n",
            "config.max_iters = 10000\n",
            "config.weight_decay=0.05\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=config.max_iters\n",
            "config.warmup_iters=0.08*config.max_iters\n",
            "config.batch_size = 2\n",
            "config.gradient_accumulation_steps = 16\n",
            "config.init_from = \"resume\"\n",
            "config.out_dir =\"models/scratch\"\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "config.eval_iters = 4\n",
            "\n",
            "\n",
            "config.wandb_project = \"shapenet_token_transformer\"\n",
            "config.metric_interval = 250\n",
            "\n",
            "cb_size = rvq.layers[0].codebook_size\n",
            "\n",
            "# BIG: model_config = GPTConfig(n_embd=512, block_size=dataset_token[0][0].size(0), n_head=16, n_layer=8, vocab_size=cb_size + 1, dropout=0.0, max_len= None)\n",
            "# SMALL: model_config = GPTConfig(n_embd=256, block_size=dataset_token[0][0].size(0), n_head=16, n_layer=6, vocab_size=cb_size + 1, dropout=0.0, max_len= None)\n",
            "# MEDIUM:\n",
            "model_config = GPTConfig(n_embd=384, block_size=dataset_token[0][0].size(0), n_head=16, n_layer=7, vocab_size=cb_size + 1, dropout=0.0, max_len= None)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "token_dict = {\n",
            "    \"SOS\": cb_size,\n",
            "}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_ratio = 0.9\n",
            "\n",
            "def create_split_indices(n, train_ratio=0.9):\n",
            "    # Generate a random permutation of indices from 0 to n-1\n",
            "    shuffled_indices = torch.randperm(n)\n",
            "    # Determine the cut-off for training data\n",
            "    train_size = int(train_ratio * n)\n",
            "    # Split indices into training and validation sets\n",
            "    train_indices = shuffled_indices[:train_size]\n",
            "    val_indices = shuffled_indices[train_size:]\n",
            "    return train_indices, val_indices\n",
            "\n",
            "train_indices, val_indices = create_split_indices(len(dataset))\n",
            "\n",
            "def get_batch_lambda(config, dataset, model_config, split, losses = None):\n",
            "    batch_size = config.batch_size\n",
            "    \n",
            "    if losses is not None:\n",
            "        assert losses.shape[0] == len(dataset)\n",
            "        training_losses = losses[:int(train_ratio * len(dataset))]\n",
            "        val_losses = losses[int(train_ratio * len(dataset)):]\n",
            "\n",
            "    # Select indices based on the split\n",
            "    if split == 'train':\n",
            "        # Randomly select batch_size indices from the train_indices\n",
            "        # indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
            "        if losses is not None:\n",
            "            probabilities = torch.softmax(training_losses, dim=0)\n",
            "            sampled_indices = torch.multinomial(probabilities, batch_size, replacement=True)\n",
            "            indices = train_indices[sampled_indices.to(train_indices)]\n",
            "        else:\n",
            "            indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
            "        \n",
            "    elif split == 'val':\n",
            "        if losses is not None:\n",
            "            probabilities = torch.softmax(val_losses, dim=0)\n",
            "            sampled_indices = torch.multinomial(probabilities, batch_size, replacement=True)\n",
            "            indices = val_indices[sampled_indices.to(val_indices)]\n",
            "        # Randomly select batch_size indices from the val_indices\n",
            "        indices = val_indices[torch.randint(0, len(val_indices), (batch_size,))]\n",
            "    \n",
            "    \n",
            "    # Initialize lists to hold the sequences and labels\n",
            "    samples = []\n",
            "    labels = []\n",
            "\n",
            "    # Collect samples and labels\n",
            "    for idx in indices:\n",
            "        sample, label = dataset[idx]\n",
            "        start_tokens = torch.Tensor([token_dict[\"SOS\"]]).long()  # Start of sequence token\n",
            "        sample = torch.cat((start_tokens.to(sample), sample.squeeze(-1)), dim=0)\n",
            "        #start_tokens = torch.Tensor([0]).long()  # Start of sequence token\n",
            "        #sample = torch.cat((start_tokens, sample + 1), dim=0)\n",
            "        samples.append(sample)\n",
            "        labels.append(label)\n",
            "\n",
            "    # Prepare the sequences for model input\n",
            "    max_len = dataset_token[0][0].size(0) + 1 #samples[0].size(0)\n",
            "    x = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    y = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    \n",
            "    for i, sample in enumerate(samples):\n",
            "        end_index = sample.size(0) - 1\n",
            "        x[i, :end_index] = sample[:-1]  # Exclude the last token for x\n",
            "        y[i, :end_index] = sample[1:]   # Exclude the first token for y\n",
            "\n",
            "    idx = torch.zeros((batch_size,), dtype=torch.long)\n",
            "    x_cutted = torch.zeros((batch_size, model_config.block_size), dtype=torch.long)\n",
            "    y_cutted = torch.zeros((batch_size, model_config.block_size), dtype=torch.long)\n",
            "\n",
            "    for i, offset in enumerate(idx):\n",
            "        x_cutted[i, :] = x[i, offset:offset + model_config.block_size]\n",
            "        y_cutted[i, :] = y[i, offset:offset + model_config.block_size]\n",
            "\n",
            "    # x and y have to be\n",
            "    x_cutted = x_cutted.to(config.device)\n",
            "    y_cutted = y_cutted.to(config.device)\n",
            "    \n",
            "    if losses is not None:\n",
            "        return x_cutted, y_cutted, idx, indices\n",
            "\n",
            "    return x_cutted, y_cutted, idx\n",
            "\n",
            "create_get_batch = lambda config, dataset, model_config: lambda split, losses = None: get_batch_lambda(config, dataset, model_config, split, losses=losses)\n",
            "get_batch = create_get_batch(config, dataset_token, model_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import wandb\n",
            "\n",
            "def custom_eval(pred: torch.Tensor, mode: str, k: int, x, y, loss=None):\n",
            "  if mode == \"val\":\n",
            "    return\n",
            "\n",
            "  if k != 1:\n",
            "    return\n",
            "\n",
            "  tokens_batch = pred.detach().argmax(dim=-1)\n",
            "\n",
            "  orig_objs = []\n",
            "  objs = []\n",
            "  losses = {}\n",
            "\n",
            "  for i in range(2):\n",
            "\n",
            "    tokens_orig = y[i]\n",
            "    model_dict = token_transform.inverse(tokens_orig.unsqueeze(-1))\n",
            "    model_quantized = model_transform(model_dict)[0]\n",
            "    mesh_orig, sdf_orig = model_to_mesh(model_quantized, res=128)\n",
            "\n",
            "    # export trimesh mesh to obj string\n",
            "    mesh_str = mesh_orig.export(file_type=\"obj\")\n",
            "\n",
            "    # create fake file for mesh_str\n",
            "    import io\n",
            "    with io.StringIO() as f:\n",
            "        f.write(mesh_str)\n",
            "        f.seek(0)\n",
            "        orig_objs.append(wandb.Object3D(f, file_type=\"obj\"))\n",
            "\n",
            "\n",
            "    tokens = tokens_batch[i]\n",
            "    # enforce that tokens do not contain SOS token, by setting it to 0\n",
            "    tokens[tokens == token_dict[\"SOS\"]] = 0\n",
            "\n",
            "    model_dict = token_transform.inverse(tokens.unsqueeze(-1))\n",
            "    model_quantized = model_transform(model_dict)[0]\n",
            "    #model = dataset_model[index][0]\n",
            "    mesh, sdf = model_to_mesh(model_quantized, res=128)\n",
            "    # export trimesh mesh to obj string\n",
            "    mesh_str = mesh.export(file_type=\"obj\")\n",
            "\n",
            "    if loss:\n",
            "      losses[f\"{i}\"] = loss[i].item()\n",
            "\n",
            "    # create fake file for mesh_str\n",
            "    with io.StringIO() as f:\n",
            "        f.write(mesh_str)\n",
            "        f.seek(0)\n",
            "        objs.append(wandb.Object3D(f, file_type=\"obj\"))\n",
            "  \n",
            "  wandb.log({\"meshes_orig\": orig_objs, \"meshes\": objs, \"losses\": losses})\n",
            "\n",
            "  del orig_objs\n",
            "  del objs\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Prepeare model parameters and train\n",
            "import wandb\n",
            "torch.cuda.empty_cache()\n",
            "trained_model = training_nano_gpt.train(get_batch, config, model_config, rvq, rq_config, token_dict=token_dict, custom_eval=custom_eval, dataset=dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from networks.nano_gpt import GPT\n",
            "from utils import get_default_device\n",
            "\n",
            "def load_model(by: str):\n",
            "  \"\"\"\n",
            "  By is expected to be \"train\" or \"val\"\n",
            "  \"\"\"\n",
            "  model_dict = torch.load(f\"./models/scratch/ckpt_best_{by}_medium_last.pt\")\n",
            "  # Configuration\n",
            "  print(model_dict.keys())\n",
            "  idx = 3\n",
            "\n",
            "  device = get_default_device()\n",
            "  model = GPT(model_dict[\"model_config\"])\n",
            "  model.to(device=device)\n",
            "  model.load_state_dict(model_dict[\"model\"])\n",
            "\n",
            "  model.eval()\n",
            "\n",
            "  vq = ResidualVQ(**model_dict[\"vq_config\"])\n",
            "  vq.load_state_dict(model_dict[\"vq_state_dict\"])\n",
            "  vq.eval()\n",
            "\n",
            "  return model, vq\n",
            "\n",
            "dataset = ShapeNetDataset(os.path.join(\"./\", \"datasets\", \"shapenet_nef_2\", \"pretrained\"), transform=token_transform)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "grid = [\n",
            "  # (1, 1),\n",
            "  (3, 0.8),\n",
            "  (3, 1),\n",
            "  (5, 0.8),\n",
            "  (5, 1)\n",
            "]\n",
            "\n",
            "\n",
            "i = 2\n",
            "model = None\n",
            "vq = None\n",
            "by = \"train\"\n",
            "\n",
            "switch_interval = 10\n",
            "\n",
            "import os\n",
            "\n",
            "# create all grid folders\n",
            "for top_k, temperature in grid:\n",
            "    for by_i in [\"train\", \"val\"]:\n",
            "      dir_name = f\"./models/generation/{by_i}_temperature_{temperature}_top_{top_k}\"\n",
            "      if not os.path.exists(dir_name):\n",
            "          os.makedirs(dir_name)\n",
            "\n",
            "\n",
            "while True:\n",
            "  if i % switch_interval == 0 or model is None:\n",
            "    by = \"train\" if by == \"val\" else \"val\"\n",
            "    model, vq = load_model(by) \n",
            "    print(f\"now generating {by}\")\n",
            "\n",
            "  for top_k, temperature in grid:\n",
            "    generated_tokens = model.generate(torch.Tensor([[token_dict[\"SOS\"]]]).long().to(device=\"cuda\"), dataset[0][0].shape[0], temperature=temperature, top_k=top_k)[:, 1:]\n",
            "    torch.save(generated_tokens, f\"./models/generation/{by}_temperature_{temperature}_top_{top_k}/generation-{i}.pt\")\n",
            "    \n",
            "  i += 1"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "import glob\n",
            "from utils.hd_utils import render_image\n",
            "files = glob.glob(\"./models/generation/gen*.pt\")\n",
            "\n",
            "for i, file in tqdm(enumerate(files)):\n",
            "  generated_tokens = torch.load(file)\n",
            "\n",
            "  model_dict = token_transform.inverse(generated_tokens.unsqueeze(-1))\n",
            "  model_quantized = model_transform(model_dict)[0]\n",
            "  #model = dataset_model[index][0]\n",
            "  mesh, sdf = model_to_mesh(model_quantized, res=256)\n",
            "  render_image(mesh, path=f\"./models/rendering/{i}.png\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokens = torch.load(files[0])\n",
            "model_dict = token_transform.inverse(tokens)\n",
            "model_quantized = model_transform(model_dict)[0]\n",
            "#model = dataset_model[index][0]\n",
            "mesh, sdf = model_to_mesh(model_quantized, res=256)\n",
            "mesh.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from utils.hd_utils import render_mesh\n",
            "\n",
            "\n",
            "color, depth = render_mesh(mesh)\n",
            "# show color as image\n",
            "plt.imshow(color)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
