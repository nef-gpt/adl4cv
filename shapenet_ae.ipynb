{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from data.neural_field_datasets_shapenet import AllWeights3D, FlattenTransform3D, ImageTransform3D, ShapeNetDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from networks.shapenet_ae import GlobalAutoencoder\n",
    "\n",
    "from networks.shapenet_ae import VanillaDecoder, VanillaEncoder, PositionEncoder\n",
    "\n",
    "\n",
    "\n",
    "dataset_weights = ShapeNetDataset(os.path.join(\"./\", \"datasets\", \"shapenet_nefs\", \"pretrained\"), transform=ImageTransform3D())\n",
    "\n",
    "input_size = 3712\n",
    "dimensions = [input_size, int(input_size//1.25), int(input_size//1.5), int(input_size//1.75), int(input_size//2)]\n",
    "encoder = VanillaEncoder(dimensions)\n",
    "dimensions.reverse()\n",
    "decoder = VanillaDecoder(dimensions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming tensor of shape [3482, 116, 32]\n",
    "tensor = torch.stack([weights for weights in dataset_weights])\n",
    "\n",
    "num_vec = tensor.shape[1]\n",
    "\n",
    "# Define the split ratio\n",
    "split_ratio = 0.8\n",
    "num_train = int(split_ratio * tensor.shape[0])\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(tensor.shape[0])\n",
    "\n",
    "# Split the indices for training and validation\n",
    "train_indices = indices[:num_train]\n",
    "val_indices = indices[num_train:]\n",
    "\n",
    "# Index the tensor to create training and validation sets\n",
    "train_data = tensor[train_indices]\n",
    "val_data = tensor[val_indices]\n",
    "\n",
    "print(f'Training data shape: {train_data.shape}')\n",
    "print(f'Validation data shape: {val_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32\n",
    "\n",
    "# local encoder\n",
    "latent_dims_local = [32] #[24]\n",
    "num_layers_enc_local_list = [2, 3]\n",
    "\n",
    "\n",
    "autoencoder_latent_dim = [32*116]\n",
    "\n",
    "emb_dims = [1, 64]\n",
    "\n",
    "num_layers_global = [2, 3, 4]\n",
    "\n",
    "\n",
    "\n",
    "for latent_dim_global in autoencoder_latent_dim:\n",
    "    for num_layer_global in num_layers_global:\n",
    "        for num_layers_enc_local in num_layers_enc_local_list:\n",
    "            for emb_dim in emb_dims:\n",
    "                    for latent_dim_local in latent_dims_local:\n",
    "                        \n",
    "                        config = {\n",
    "                            \"input_dim\": input_dim,\n",
    "                            # local encoder\n",
    "                            \"latent_dim_local\": latent_dim_local,\n",
    "                            \"num_layers_enc_local\": num_layers_enc_local,\n",
    "                            # global encoder\n",
    "                            \"latent_dim_global\": latent_dim_global,\n",
    "                            \"num_layer_enc_global\": num_layer_global,\n",
    "                            # global decoder\n",
    "                            \"num_layers_dec_global\": num_layer_global,\n",
    "                            \"emb_dim_local\": emb_dim,\n",
    "                            \"num_vec_local\": num_vec,      \n",
    "                        }\n",
    "                        \n",
    "                        model = GlobalAutoencoder(**config)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 64)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=96, out_features=56, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=56, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 64)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=80, out_features=56, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=56, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.3406434953212738 split (0.913, 0.087): 100%|██████████| 175/175 [00:14<00:00, 12.23it/s]             \n",
      "Avg. Loss 0.1345338523387909 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.38it/s]      \n",
      "Avg. Loss 0.1110081672668457 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 13.22it/s] \n",
      "Avg. Loss 0.09851808845996857 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.56it/s]\n",
      "Avg. Loss 0.09111965447664261 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.55it/s]\n",
      "Avg. Loss 0.08379268646240234 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.71it/s]\n",
      "Avg. Loss 0.0837230235338211 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.44it/s] \n",
      "Avg. Loss 0.08161348849534988 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.51it/s]\n",
      "Avg. Loss 0.08715872466564178 split (0.9, 0.1): 100%|██████████| 175/175 [00:11<00:00, 14.60it/s]\n",
      "Avg. Loss 0.07588676363229752 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 13.43it/s]\n",
      "Avg. Loss 0.08257369697093964 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.59it/s]\n",
      "Avg. Loss 0.08038660138845444 split (0.9, 0.1):  43%|████▎     | 75/175 [00:06<00:08, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 32, 'latent_dim_local': 16, 'num_layers_enc_local': 2, 'latent_dim_global': 1856, 'num_layer_enc_global': 2, 'num_layers_dec_global': 2, 'emb_dim_local': 64, 'num_vec_local': 116, 'only_global_decode': True}\n",
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 128)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=160, out_features=88, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=88, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 128)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=144, out_features=88, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=88, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.2282196432352066 split (0.913, 0.087): 100%|██████████| 175/175 [01:28<00:00,  1.98it/s]              \n",
      "Avg. Loss 0.111301489174366 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 12.51it/s]       \n",
      "Avg. Loss 0.08954267203807831 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.35it/s]\n",
      "Avg. Loss 0.0796278715133667 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.02it/s] \n",
      "Avg. Loss 0.07925333082675934 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.45it/s]\n",
      "Avg. Loss 0.07724597305059433 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.60it/s]\n",
      "Avg. Loss 0.07713853567838669 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.84it/s]\n",
      "Avg. Loss 0.08295440673828125 split (0.9, 0.1): 100%|██████████| 175/175 [00:11<00:00, 14.71it/s]\n",
      "Avg. Loss 0.07804232835769653 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 13.65it/s]\n",
      "Avg. Loss 0.08287503570318222 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 13.39it/s]\n",
      "Avg. Loss 0.07679444551467896 split (0.9, 0.1): 100%|██████████| 175/175 [00:12<00:00, 14.03it/s]\n",
      "Avg. Loss 0.07259208709001541 split (0.9, 0.1):  43%|████▎     | 75/175 [00:05<00:07, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 32, 'latent_dim_local': 16, 'num_layers_enc_local': 2, 'latent_dim_global': 1856, 'num_layer_enc_global': 2, 'num_layers_dec_global': 2, 'emb_dim_local': 128, 'num_vec_local': 116, 'only_global_decode': True}\n",
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 256)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=288, out_features=152, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=152, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 256)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=272, out_features=152, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=152, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.17148073017597198 split (0.913, 0.087): 100%|██████████| 175/175 [00:16<00:00, 10.73it/s]             \n",
      "Avg. Loss 0.08757571130990982 split (0.9, 0.1): 100%|██████████| 175/175 [01:26<00:00,  2.01it/s]     \n",
      "Avg. Loss 0.0776762068271637 split (0.9, 0.1): 100%|██████████| 175/175 [00:19<00:00,  8.83it/s] \n",
      "Avg. Loss 0.08828021585941315 split (0.9, 0.1): 100%|██████████| 175/175 [00:18<00:00,  9.54it/s]\n",
      "Avg. Loss 0.07314692437648773 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 13.20it/s]\n",
      "Avg. Loss 0.07379056513309479 split (0.9, 0.1): 100%|██████████| 175/175 [00:14<00:00, 11.68it/s]\n",
      "Avg. Loss 0.0725371465086937 split (0.9, 0.1): 100%|██████████| 175/175 [00:15<00:00, 11.51it/s] \n",
      "Avg. Loss 0.07358840107917786 split (0.9, 0.1): 100%|██████████| 175/175 [00:16<00:00, 10.88it/s]\n",
      "Avg. Loss 0.07755842059850693 split (0.9, 0.1): 100%|██████████| 175/175 [00:16<00:00, 10.90it/s]\n",
      "Avg. Loss 0.06854362785816193 split (0.9, 0.1): 100%|██████████| 175/175 [00:17<00:00,  9.89it/s]\n",
      "Avg. Loss 0.06761982291936874 split (0.9, 0.1): 100%|██████████| 175/175 [00:20<00:00,  8.61it/s]\n",
      "Avg. Loss 0.0701986700296402 split (0.9, 0.1):  43%|████▎     | 75/175 [00:08<00:11,  8.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 32, 'latent_dim_local': 16, 'num_layers_enc_local': 2, 'latent_dim_global': 1856, 'num_layer_enc_global': 2, 'num_layers_dec_global': 2, 'emb_dim_local': 256, 'num_vec_local': 116, 'only_global_decode': True}\n",
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 512)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=544, out_features=280, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=280, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 512)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=528, out_features=280, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=280, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.15787476301193237 split (0.913, 0.087): 100%|██████████| 175/175 [00:26<00:00,  6.60it/s]             \n",
      "Avg. Loss 0.0814746767282486 split (0.9, 0.1): 100%|██████████| 175/175 [00:20<00:00,  8.48it/s]      \n",
      "Avg. Loss 0.0755898505449295 split (0.9, 0.1): 100%|██████████| 175/175 [00:22<00:00,  7.84it/s] \n",
      "Avg. Loss 0.07231734693050385 split (0.9, 0.1): 100%|██████████| 175/175 [00:27<00:00,  6.40it/s]\n",
      "Avg. Loss 0.07872050255537033 split (0.9, 0.1): 100%|██████████| 175/175 [00:22<00:00,  7.90it/s]\n",
      "Avg. Loss 0.06980642676353455 split (0.9, 0.1): 100%|██████████| 175/175 [00:18<00:00,  9.42it/s]\n",
      "Avg. Loss 0.06942547857761383 split (0.9, 0.1): 100%|██████████| 175/175 [00:19<00:00,  8.86it/s]\n",
      "Avg. Loss 0.07027234137058258 split (0.9, 0.1): 100%|██████████| 175/175 [00:19<00:00,  8.86it/s]\n",
      "Avg. Loss 0.06385580450296402 split (0.9, 0.1): 100%|██████████| 175/175 [00:20<00:00,  8.67it/s]\n",
      "Avg. Loss 0.06243082135915756 split (0.9, 0.1): 100%|██████████| 175/175 [00:21<00:00,  8.25it/s]\n",
      "Avg. Loss 0.0628613829612732 split (0.9, 0.1): 100%|██████████| 175/175 [00:18<00:00,  9.38it/s] \n",
      "Avg. Loss 0.06555520743131638 split (0.9, 0.1):  43%|████▎     | 75/175 [00:09<00:12,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 32, 'latent_dim_local': 16, 'num_layers_enc_local': 2, 'latent_dim_global': 1856, 'num_layer_enc_global': 2, 'num_layers_dec_global': 2, 'emb_dim_local': 512, 'num_vec_local': 116, 'only_global_decode': True}\n",
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 64)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=96, out_features=69, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=69, out_features=43, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=43, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 64)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=64, out_features=48, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=48, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.3793942928314209 split (0.913, 0.087): 100%|██████████| 175/175 [00:16<00:00, 10.45it/s]             \n",
      "Avg. Loss 0.15212559700012207 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 12.86it/s]     \n",
      "Avg. Loss 0.11776819080114365 split (0.9, 0.1): 100%|██████████| 175/175 [00:14<00:00, 11.70it/s]\n",
      "Avg. Loss 0.0959445908665657 split (0.9, 0.1): 100%|██████████| 175/175 [00:16<00:00, 10.41it/s] \n",
      "Avg. Loss 0.08959411829710007 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 12.71it/s]\n",
      "Avg. Loss 0.08385311812162399 split (0.9, 0.1): 100%|██████████| 175/175 [00:14<00:00, 11.78it/s]\n",
      "Avg. Loss 0.08855654299259186 split (0.9, 0.1): 100%|██████████| 175/175 [00:13<00:00, 12.71it/s]\n",
      "Avg. Loss 0.08104921132326126 split (0.9, 0.1): 100%|██████████| 175/175 [00:15<00:00, 11.01it/s]\n",
      "Avg. Loss 0.08004723489284515 split (0.9, 0.1): 100%|██████████| 175/175 [00:21<00:00,  8.20it/s]\n",
      "Avg. Loss 0.08007019758224487 split (0.9, 0.1): 100%|██████████| 175/175 [00:22<00:00,  7.80it/s]\n",
      "Avg. Loss 0.08280591666698456 split (0.9, 0.1): 100%|██████████| 175/175 [00:16<00:00, 10.48it/s]\n",
      "Avg. Loss 0.08504221588373184 split (0.9, 0.1):  43%|████▎     | 75/175 [00:08<00:11,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 32, 'latent_dim_local': 16, 'num_layers_enc_local': 3, 'latent_dim_global': 1856, 'num_layer_enc_global': 2, 'num_layers_dec_global': 2, 'emb_dim_local': 64, 'num_vec_local': 116, 'only_global_decode': True}\n",
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 128)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=160, out_features=112, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=112, out_features=64, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 128)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=144, out_features=107, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=107, out_features=69, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=69, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.21710175275802612 split (0.913, 0.087): 100%|██████████| 175/175 [00:29<00:00,  5.84it/s]             \n",
      "Avg. Loss 0.10726000368595123 split (0.9, 0.1): 100%|██████████| 175/175 [00:21<00:00,  8.15it/s]     \n",
      "Avg. Loss 0.09101463854312897 split (0.9, 0.1): 100%|██████████| 175/175 [00:19<00:00,  8.97it/s]\n",
      "Avg. Loss 0.08633514493703842 split (0.9, 0.1): 100%|██████████| 175/175 [00:20<00:00,  8.42it/s]\n",
      "Avg. Loss 0.07535450905561447 split (0.9, 0.1): 100%|██████████| 175/175 [00:21<00:00,  8.03it/s]\n",
      "Avg. Loss 0.08257859945297241 split (0.9, 0.1): 100%|██████████| 175/175 [00:19<00:00,  8.93it/s]\n",
      "Avg. Loss 0.07806489616632462 split (0.9, 0.1): 100%|██████████| 175/175 [00:17<00:00, 10.23it/s]\n",
      "Avg. Loss 0.07491916418075562 split (0.9, 0.1): 100%|██████████| 175/175 [00:15<00:00, 11.23it/s]\n",
      "Avg. Loss 0.07799524068832397 split (0.9, 0.1): 100%|██████████| 175/175 [00:15<00:00, 11.49it/s]\n",
      "Avg. Loss 0.07448960840702057 split (0.9, 0.1): 100%|██████████| 175/175 [00:14<00:00, 12.28it/s]\n",
      "Avg. Loss 0.07293510437011719 split (0.9, 0.1): 100%|██████████| 175/175 [00:18<00:00,  9.23it/s]\n",
      "Avg. Loss 0.0717504695057869 split (0.9, 0.1):  43%|████▎     | 75/175 [00:06<00:08, 12.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 32, 'latent_dim_local': 16, 'num_layers_enc_local': 3, 'latent_dim_global': 1856, 'num_layer_enc_global': 2, 'num_layers_dec_global': 2, 'emb_dim_local': 128, 'num_vec_local': 116, 'only_global_decode': True}\n",
      "GlobalAutoencoder(\n",
      "  (local_autoencoder): VanillaAutoencoder(\n",
      "    (encoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 256)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=288, out_features=197, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=197, out_features=107, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=107, out_features=16, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): PositionEncoder(\n",
      "      (emb): Embedding(116, 256)\n",
      "      (encoder): Sequential(\n",
      "        (0): Linear(in_features=272, out_features=192, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=192, out_features=112, bias=True)\n",
      "        (3): GELU(approximate='none')\n",
      "        (4): Linear(in_features=112, out_features=32, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flattened_encoder): Identity()\n",
      "  (decoder): VanillaDecoder(\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=1856, out_features=2784, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=2784, out_features=3712, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss 0.1685052514076233 split (0.913, 0.087): 100%|██████████| 175/175 [00:16<00:00, 10.56it/s]              \n",
      "Avg. Loss 0.09335536509752274 split (0.9, 0.1): 100%|██████████| 175/175 [00:18<00:00,  9.34it/s]     \n",
      "Avg. Loss 0.08205533772706985 split (0.9, 0.1): 100%|██████████| 175/175 [00:18<00:00,  9.31it/s]\n",
      "Avg. Loss 0.0757419690489769 split (0.9, 0.1): 100%|██████████| 175/175 [00:17<00:00, 10.26it/s] \n",
      "Avg. Loss 0.0761820450425148 split (0.9, 0.1): 100%|██████████| 175/175 [00:23<00:00,  7.50it/s] \n",
      "Avg. Loss 0.07536119222640991 split (0.9, 0.1): 100%|██████████| 175/175 [00:21<00:00,  8.20it/s]\n",
      "Avg. Loss 0.07377185672521591 split (0.9, 0.1):  62%|██████▏   | 108/175 [00:20<00:12,  5.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m local_loss, global_loss, splits \u001b[38;5;241m=\u001b[39m interpolate_loss(\u001b[38;5;28miter\u001b[39m, lr_decay_iters, local_loss, global_loss, max_loss_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m local_loss \u001b[38;5;241m+\u001b[39m global_loss\n\u001b[0;32m--> 125\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exp_avg_loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/uni/adl4cv/adl4cv/adl4cv/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/adl4cv/adl4cv/adl4cv/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/adl4cv/adl4cv/adl4cv/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import math\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return 0.0\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (\n",
    "        lr_decay_iters - warmup_iters\n",
    "    )\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return coeff * (learning_rate)\n",
    "\n",
    "def interpolate_loss(iter, max_iter, loss_1, loss_2, max_loss_2 = 1.0):\n",
    "    \n",
    "    split_1, split_2 =  1 - iter / max_iter, iter / max_iter\n",
    "    \n",
    "    if split_2 > max_loss_2:\n",
    "        split_1 = 1 - max_loss_2\n",
    "        split_2 = max_loss_2\n",
    "    \n",
    "    return loss_1 * split_1, loss_2 * split_2, (split_1, split_2)\n",
    "    \n",
    "     \n",
    "input_dim = 32\n",
    "\n",
    "# local encoder\n",
    "latent_dims_local = [16]\n",
    "num_layers_enc_local_list = [2, 3]\n",
    "\n",
    "\n",
    "autoencoder_latent_dim = [int(3712//2)]\n",
    "\n",
    "emb_dims = [64, 128, 256, 512]\n",
    "\n",
    "num_layers_global = [2, 3]\n",
    "\n",
    "\n",
    "\n",
    "for latent_dim_global in autoencoder_latent_dim:\n",
    "    for num_layer_global in num_layers_global:\n",
    "        for num_layers_enc_local in num_layers_enc_local_list:\n",
    "            for emb_dim in emb_dims:\n",
    "                    for latent_dim_local in latent_dims_local:\n",
    "                        \n",
    "                        config = {\n",
    "                            \"input_dim\": input_dim,\n",
    "                            # local encoder\n",
    "                            \"latent_dim_local\": latent_dim_local,\n",
    "                            \"num_layers_enc_local\": num_layers_enc_local,\n",
    "                            # global encoder\n",
    "                            \"latent_dim_global\": latent_dim_global,\n",
    "                            \"num_layer_enc_global\": num_layer_global,\n",
    "                            # global decoder\n",
    "                            \"num_layers_dec_global\": num_layer_global,\n",
    "                            \"emb_dim_local\": emb_dim,\n",
    "                            \"num_vec_local\": num_vec,  \n",
    "                            \"only_global_decode\": True,    \n",
    "                        }\n",
    "                        \n",
    "                        model = GlobalAutoencoder(**config)\n",
    "                        print(model)\n",
    "\n",
    "                        \n",
    "                        lr_decay_iters = 2000\n",
    "                        warmup_iters = 0.05 * lr_decay_iters\n",
    "                        learning_rate = 0.001\n",
    "\n",
    "                        eval_iters = 4\n",
    "\n",
    "                        wandb.init(project=\"autoencoder\", name=f\"GRID SEARCH LOCAL+GLOBAL ## {config} ##\")\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        criterion = nn.MSELoss()\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                        # Training the autoencoder\n",
    "                        num_epochs = lr_decay_iters\n",
    "                        batch_size = 16\n",
    "                        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "                        val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "                        exp_avg_loss = None\n",
    "\n",
    "                        iter = 0\n",
    "\n",
    "                        for epoch in range(num_epochs):\n",
    "                            batch_bat = tqdm(train_dataloader)\n",
    "                            for batch in batch_bat:\n",
    "                                if iter%eval_iters == 0:\n",
    "                                    for batch in val_dataloader:\n",
    "                                        model.eval()\n",
    "                                        reconstructed_global, reconstructed_local = model(batch)\n",
    "                                        local_loss = criterion(reconstructed_local, batch[:, :, :-1])\n",
    "                                        global_loss = criterion(reconstructed_global, torch.flatten(batch[:, :, :-1], start_dim=1))\n",
    "                                        local_loss, global_loss, splits = interpolate_loss(iter, lr_decay_iters, local_loss, global_loss, max_loss_2=0.05)\n",
    "                                        \n",
    "                                        val_loss = local_loss + global_loss\n",
    "                                        model.train() \n",
    "                                        break               \n",
    "                                \n",
    "                                lr = get_lr(iter)\n",
    "                                for param_group in optimizer.param_groups:\n",
    "                                    param_group[\"lr\"] = lr\n",
    "                                optimizer.zero_grad()\n",
    "                                reconstructed_global, reconstructed_local = model(batch)\n",
    "                                local_loss = criterion(reconstructed_local, batch[:, :, :-1])\n",
    "                                global_loss = criterion(reconstructed_global, torch.flatten(batch[:, :, :-1], start_dim=1))\n",
    "                                local_loss, global_loss, splits = interpolate_loss(iter, lr_decay_iters, local_loss, global_loss, max_loss_2=0.05)\n",
    "\n",
    "                                loss = local_loss + global_loss\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                                if exp_avg_loss == None:\n",
    "                                    exp_avg_loss = loss\n",
    "                                exp_avg_loss = 0.95*exp_avg_loss + 0.05*loss\n",
    "                                batch_bat.set_description(f\"Avg. Loss {exp_avg_loss} split {splits}\")\n",
    "                                wandb.log({\"iter\": iter, \"loss\": loss.item(), \"local_loss\": local_loss, \"global_loss\": global_loss, \"val_loss\": val_loss, \"epoch\": epoch, \"lr\": lr})\n",
    "                                iter += 1\n",
    "                                \n",
    "                                if iter > lr_decay_iters:\n",
    "                                    break\n",
    "                                \n",
    "                            if iter > lr_decay_iters:\n",
    "                                break\n",
    "                        \n",
    "                        print(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local encoder\n",
    "latent_dims_local = [24]\n",
    "num_layers_enc_local_list = [2, 3]\n",
    "\n",
    "\n",
    "autoencoder_latent_dim = [int(3712//2)]\n",
    "\n",
    "emb_dims = [256, 512]\n",
    "\n",
    "num_layers_global = [2, 3]\n",
    "\n",
    "config = {\n",
    "        \"input_dim\": input_dim,\n",
    "        # local encoder\n",
    "        \"latent_dim_local\": latent_dim_local,\n",
    "        \"num_layers_enc_local\": num_layers_enc_local,\n",
    "        # global encoder\n",
    "        \"latent_dim_global\": latent_dim_global,\n",
    "        \"num_layer_enc_global\": num_layer_global,\n",
    "        # global decoder\n",
    "        \"num_layers_dec_global\": num_layer_global,\n",
    "        \"emb_dim_local\": emb_dim,\n",
    "        \"num_vec_local\": num_vec,      \n",
    "    } \n",
    "    \n",
    "model = GlobalAutoencoder(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset_weights, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "for key in dataset_weights[0][0][\"state_dict\"].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
