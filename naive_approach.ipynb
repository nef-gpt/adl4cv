{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from data.DWSNets_dataset import DWSNetsDataset\n",
    "from networks.regression_transformer import RegressionTransformerConfig\n",
    "from training.naive_approach import train, Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "config = Config()\n",
    "model_config = RegressionTransformerConfig(n_embd=1, block_size=1024, n_head=1)\n",
    "\n",
    "\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luca/uni/master/second-semester/adl4cv/datasets/DWSNets/mnist-inrs/mnist_splits.json\n"
     ]
    }
   ],
   "source": [
    "# Dataloading\n",
    "import torch.utils\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(dir_path, \"adl4cv\", \"datasets\", \"DWSNets\", \"mnist-inrs\")\n",
    "\n",
    "dataset = DWSNetsDataset(data_root)\n",
    "path = \"datasets/DWSNets/mnist-inrs/mnist_splits.json\"\n",
    "dataset_map = json.load(open(path, \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def flatten_weights(weights: OrderedDict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Flatten the weights of a model\n",
    "    :param weights: Ordered dictionary of weights (string -> tensor)\n",
    "    :return: The flattened weights (single tensor)\n",
    "    \"\"\"\n",
    "    return torch.cat([torch.flatten(v) for k, v in weights.items()])\n",
    "\n",
    "\n",
    "weights = torch.load(dataset_map[\"train\"][\"path\"][0], map_location=torch.device('cpu'))\n",
    "flattened = flatten_weights(weights)\n",
    "\n",
    "\n",
    "def get_batch(split: str):\n",
    "    # let's get a batch with the single element\n",
    "    # y should be the same shifted by 1\n",
    "    ix = torch.randint(torch.numel(flattened) - model_config.block_size, (config.batch_size,))\n",
    "    x = torch.stack(\n",
    "        [flattened[i : i + model_config.block_size] for i in ix]\n",
    "    )\n",
    "    y = torch.stack(\n",
    "        [flattened[i + 1 : i + 1 + model_config.block_size] for i in ix]\n",
    "    )\n",
    "    # x and y have to be (1, *, 1)\n",
    "    x = x.unsqueeze(-1)\n",
    "    y = y.unsqueeze(-1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "num decayed parameter tensors: 50, with 1,169 parameters\n",
      "num non-decayed parameter tensors: 98, with 158 parameters\n",
      "using fused AdamW: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ke1c9ekg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁██</td></tr><tr><td>mfu</td><td>▁▁▁</td></tr><tr><td>train/loss</td><td>▆█▁</td></tr><tr><td>val/loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>4000</td></tr><tr><td>lr</td><td>0.0006</td></tr><tr><td>mfu</td><td>-100.0</td></tr><tr><td>train/loss</td><td>0.00035</td></tr><tr><td>val/loss</td><td>0.00035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run-2024-05-15-19-13-36</strong> at: <a href='https://wandb.ai/adl-for-cv/regression-transformer/runs/ke1c9ekg' target=\"_blank\">https://wandb.ai/adl-for-cv/regression-transformer/runs/ke1c9ekg</a><br/> View project at: <a href='https://wandb.ai/adl-for-cv/regression-transformer' target=\"_blank\">https://wandb.ai/adl-for-cv/regression-transformer</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240515_191336-ke1c9ekg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ke1c9ekg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/luca/uni/master/second-semester/adl4cv/wandb/run-20240515_192755-qqikh3a0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adl-for-cv/regression-transformer/runs/qqikh3a0' target=\"_blank\">run-2024-05-15-19-27-55</a></strong> to <a href='https://wandb.ai/adl-for-cv/regression-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adl-for-cv/regression-transformer' target=\"_blank\">https://wandb.ai/adl-for-cv/regression-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adl-for-cv/regression-transformer/runs/qqikh3a0' target=\"_blank\">https://wandb.ai/adl-for-cv/regression-transformer/runs/qqikh3a0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.0004, val loss 0.0003\n",
      "step 100: train loss 0.0004, val loss 0.0004\n",
      "step 200: train loss 0.0004, val loss 0.0003\n",
      "saving checkpoint to models\n",
      "step 300: train loss 0.0004, val loss 0.0003\n",
      "saving checkpoint to models\n",
      "step 400: train loss 0.0004, val loss 0.0004\n",
      "step 500: train loss 0.0004, val loss 0.0004\n",
      "step 600: train loss 0.0004, val loss 0.0003\n",
      "step 700: train loss 0.0004, val loss 0.0004\n",
      "step 800: train loss 0.0004, val loss 0.0004\n",
      "step 900: train loss 0.0004, val loss 0.0004\n",
      "step 1000: train loss 0.0004, val loss 0.0004\n",
      "step 1100: train loss 0.0003, val loss 0.0004\n",
      "step 1200: train loss 0.0004, val loss 0.0003\n",
      "saving checkpoint to models\n",
      "step 1300: train loss 0.0003, val loss 0.0003\n",
      "step 1400: train loss 0.0004, val loss 0.0004\n",
      "step 1500: train loss 0.0003, val loss 0.0003\n",
      "step 1600: train loss 0.0004, val loss 0.0004\n",
      "step 1700: train loss 0.0004, val loss 0.0003\n",
      "saving checkpoint to models\n",
      "step 1800: train loss 0.0003, val loss 0.0003\n",
      "step 1900: train loss 0.0004, val loss 0.0004\n",
      "step 2000: train loss 0.0003, val loss 0.0003\n",
      "step 2100: train loss 0.0004, val loss 0.0004\n",
      "step 2200: train loss 0.0004, val loss 0.0003\n",
      "step 2300: train loss 0.0004, val loss 0.0003\n",
      "step 2400: train loss 0.0004, val loss 0.0004\n",
      "step 2500: train loss 0.0004, val loss 0.0003\n",
      "step 2600: train loss 0.0004, val loss 0.0003\n",
      "step 2700: train loss 0.0003, val loss 0.0003\n",
      "step 2800: train loss 0.0004, val loss 0.0004\n",
      "step 2900: train loss 0.0004, val loss 0.0003\n",
      "step 3000: train loss 0.0004, val loss 0.0004\n",
      "step 3100: train loss 0.0003, val loss 0.0003\n",
      "step 3200: train loss 0.0004, val loss 0.0004\n",
      "step 3300: train loss 0.0003, val loss 0.0004\n",
      "step 3400: train loss 0.0004, val loss 0.0004\n",
      "step 3500: train loss 0.0003, val loss 0.0004\n",
      "step 3600: train loss 0.0003, val loss 0.0003\n",
      "step 3700: train loss 0.0004, val loss 0.0004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepeare model parameters and train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/master/second-semester/adl4cv/training/naive_approach.py:232\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(get_batch, config, model_config)\u001b[0m\n\u001b[1;32m    230\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# clip the gradient\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "File \u001b[0;32m~/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepeare model parameters and train\n",
    "train(get_batch, config, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
