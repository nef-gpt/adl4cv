{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "3.10.14 (main, May 26 2024, 13:34:58) [GCC 13.2.1 20230801]\n"
               ]
            }
         ],
         "source": [
            "# log python version\n",
            "import sys\n",
            "print(sys.version)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/luca/.cache/pypoetry/virtualenvs/adl4cv-OvNqwVNf-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            }
         ],
         "source": [
            "from training import naive_approach\n",
            "from networks.regression_transformer import RegressionTransformerConfig, RegressionTransformer\n",
            "\n",
            "from data.neural_field_datasets import DWSNetsDataset, MnistNeFDataset, FlattenTransform, MinMaxTransform\n",
            "\n",
            "import os\n",
            "import torch\n",
            "import torchinfo"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Dataloading\n",
            "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
            "data_root_dwsnet = os.path.join(dir_path, \"adl4cv\", \"datasets\", \"DWSNets\", \"mnist-inrs\")\n",
            "data_root_ours = os.path.join(dir_path, \"adl4cv\", \"datasets\", \"mnist-nerfs\")\n",
            "\n",
            "class FlattenMinMaxTransform(torch.nn.Module):\n",
            "  def __init__(self, min_max: tuple = None):\n",
            "    super().__init__()\n",
            "    self.flatten = FlattenTransform()\n",
            "    if min_max:\n",
            "      self.minmax = MinMaxTransform(*min_max)\n",
            "    else:\n",
            "      self.minmax = MinMaxTransform()\n",
            "\n",
            "  def forward(self, x, y):\n",
            "    x, _ = self.flatten(x, y)\n",
            "    x, _ = self.minmax(x, y)\n",
            "    return x, y\n",
            "\n",
            "\n",
            "dataset = DWSNetsDataset(data_root_dwsnet, transform=FlattenMinMaxTransform())\n",
            "dataset_wo_min_max = DWSNetsDataset(data_root_dwsnet, transform=FlattenTransform())\n",
            "dataset_no_transform = DWSNetsDataset(data_root_dwsnet)\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": 5,\n",
            "}\n",
            "\n",
            "\n",
            "dataset_wo_min_max = MnistNeFDataset(data_root_ours, transform=FlattenTransform(), **kwargs)\n",
            "min_ours, max_ours = dataset_wo_min_max.min_max()\n",
            "dataset = MnistNeFDataset(data_root_ours, transform=FlattenMinMaxTransform((min_ours, max_ours)), **kwargs)\n",
            "dataset_no_transform = MnistNeFDataset(data_root_ours, **kwargs)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "409"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "len(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "config = naive_approach.Config()\n",
            "config.learning_rate=5e-4\n",
            "config.max_iters = 14000\n",
            "config.weight_decay=0\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=14000\n",
            "config.warmup_iters=0.1*config.max_iters\n",
            "config.batch_size = 1\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "\n",
            "# Config Transforemer\n",
            "model_config = RegressionTransformerConfig(n_embd=32, block_size=len(dataset[0][0]) - 1, n_head=8, n_layer=16)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "# take first n samples that have label == 1 (where label is second entry of dataset object)\n",
            "n = 5\n",
            "samples = [(i, dataset[i][0]) for i in range(len(dataset)) if dataset[i][1] == 5][:n]\n",
            "\n",
            "\n",
            "def get_batch(split: str):\n",
            "    # let's get a batch with the single element\n",
            "    # y should be the same shifted by 1\n",
            "    ix = torch.zeros(config.batch_size, dtype=torch.int)\n",
            "    #torch.randint(torch.numel(flattened) - model_config.block_size, (config.batch_size,))\n",
            "\n",
            "    # randomly select a sample (0...n-1)\n",
            "    split_start = 0 if split == \"train\" else int(0.8 * n)\n",
            "    split_end = int(0.8 * n) if split == \"train\" else n\n",
            "\n",
            "    sample = samples[torch.randint(split_start, split_end, (1,))][1]\n",
            "\n",
            "    x = torch.stack(\n",
            "        [sample[i : i + model_config.block_size] for i in ix]\n",
            "    )\n",
            "    y = torch.stack(\n",
            "        [sample[i + 1 : i + 1 + model_config.block_size] for i in ix]\n",
            "    )\n",
            "\n",
            "    # x and y have to be (1, *, 1)\n",
            "    x = x.unsqueeze(-1).to(config.device)\n",
            "    y = y.unsqueeze(-1).to(config.device)\n",
            "    return x, y"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Initializing a new model from scratch\n",
                  "num decayed parameter tensors: 67, with 215,616 parameters\n",
                  "num non-decayed parameter tensors: 131, with 6,752 parameters\n",
                  "using fused AdamW: True\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 0: train loss 0.9639, val loss 0.9633\n",
                  "saving checkpoint to models\n",
                  "step 100: train loss 0.3275, val loss 0.3255\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 200: train loss 0.0824, val loss 0.0804\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 300: train loss 0.0560, val loss 0.0556\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 400: train loss 0.0408, val loss 0.0415\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 500: train loss 0.0314, val loss 0.0323\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 600: train loss 0.0264, val loss 0.0290\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 700: train loss 0.0290, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 800: train loss 0.0283, val loss 0.0292\n",
                  "saving checkpoint to models\n",
                  "step 900: train loss 0.0238, val loss 0.0246\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 1000: train loss 0.0249, val loss 0.0255\n",
                  "saving checkpoint to models\n",
                  "step 1100: train loss 0.0266, val loss 0.0296\n",
                  "saving checkpoint to models\n",
                  "step 1200: train loss 0.0240, val loss 0.0271\n",
                  "saving checkpoint to models\n",
                  "step 1300: train loss 0.0271, val loss 0.0283\n",
                  "saving checkpoint to models\n",
                  "step 1400: train loss 0.0238, val loss 0.0263\n",
                  "saving checkpoint to models\n",
                  "step 1500: train loss 0.0266, val loss 0.0281\n",
                  "saving checkpoint to models\n",
                  "step 1600: train loss 0.0234, val loss 0.0262\n",
                  "saving checkpoint to models\n",
                  "step 1700: train loss 0.0243, val loss 0.0245\n",
                  "saving checkpoint to models\n",
                  "saving checkpoint to models\n",
                  "step 1800: train loss 0.0257, val loss 0.0281\n",
                  "saving checkpoint to models\n",
                  "step 1900: train loss 0.0255, val loss 0.0266\n",
                  "saving checkpoint to models\n",
                  "step 2000: train loss 0.0252, val loss 0.0249\n",
                  "saving checkpoint to models\n",
                  "step 2100: train loss 0.0246, val loss 0.0263\n",
                  "saving checkpoint to models\n",
                  "step 2200: train loss 0.0234, val loss 0.0251\n",
                  "saving checkpoint to models\n",
                  "step 2300: train loss 0.0238, val loss 0.0278\n",
                  "saving checkpoint to models\n",
                  "step 2400: train loss 0.0217, val loss 0.0264\n",
                  "saving checkpoint to models\n",
                  "step 2500: train loss 0.0234, val loss 0.0258\n",
                  "saving checkpoint to models\n",
                  "step 2600: train loss 0.0244, val loss 0.0289\n",
                  "saving checkpoint to models\n",
                  "step 2700: train loss 0.0230, val loss 0.0275\n",
                  "saving checkpoint to models\n",
                  "step 2800: train loss 0.0221, val loss 0.0245\n",
                  "saving checkpoint to models\n",
                  "step 2900: train loss 0.0220, val loss 0.0249\n",
                  "saving checkpoint to models\n",
                  "step 3000: train loss 0.0217, val loss 0.0256\n",
                  "saving checkpoint to models\n",
                  "step 3100: train loss 0.0226, val loss 0.0258\n",
                  "saving checkpoint to models\n",
                  "step 3200: train loss 0.0232, val loss 0.0277\n",
                  "saving checkpoint to models\n",
                  "step 3300: train loss 0.0226, val loss 0.0270\n",
                  "saving checkpoint to models\n",
                  "step 3400: train loss 0.0194, val loss 0.0258\n",
                  "saving checkpoint to models\n",
                  "step 3500: train loss 0.0218, val loss 0.0279\n",
                  "saving checkpoint to models\n",
                  "step 3600: train loss 0.0215, val loss 0.0273\n",
                  "saving checkpoint to models\n",
                  "step 3700: train loss 0.0194, val loss 0.0261\n",
                  "saving checkpoint to models\n",
                  "step 3800: train loss 0.0186, val loss 0.0257\n",
                  "saving checkpoint to models\n",
                  "step 3900: train loss 0.0186, val loss 0.0268\n",
                  "saving checkpoint to models\n",
                  "step 4000: train loss 0.0188, val loss 0.0281\n",
                  "saving checkpoint to models\n",
                  "step 4100: train loss 0.0181, val loss 0.0274\n",
                  "saving checkpoint to models\n",
                  "step 4200: train loss 0.0187, val loss 0.0271\n",
                  "saving checkpoint to models\n",
                  "step 4300: train loss 0.0157, val loss 0.0264\n",
                  "saving checkpoint to models\n",
                  "step 4400: train loss 0.0164, val loss 0.0269\n",
                  "saving checkpoint to models\n",
                  "step 4500: train loss 0.0155, val loss 0.0268\n",
                  "saving checkpoint to models\n",
                  "step 4600: train loss 0.0151, val loss 0.0272\n",
                  "saving checkpoint to models\n",
                  "step 4700: train loss 0.0141, val loss 0.0269\n",
                  "saving checkpoint to models\n",
                  "step 4800: train loss 0.0146, val loss 0.0273\n",
                  "saving checkpoint to models\n",
                  "step 4900: train loss 0.0157, val loss 0.0291\n",
                  "saving checkpoint to models\n",
                  "step 5000: train loss 0.0117, val loss 0.0278\n",
                  "saving checkpoint to models\n",
                  "step 5100: train loss 0.0131, val loss 0.0278\n",
                  "saving checkpoint to models\n",
                  "step 5200: train loss 0.0118, val loss 0.0281\n",
                  "saving checkpoint to models\n",
                  "step 5300: train loss 0.0096, val loss 0.0278\n",
                  "saving checkpoint to models\n",
                  "step 5400: train loss 0.0133, val loss 0.0296\n",
                  "saving checkpoint to models\n",
                  "step 5500: train loss 0.0104, val loss 0.0283\n",
                  "saving checkpoint to models\n",
                  "step 5600: train loss 0.0095, val loss 0.0279\n",
                  "saving checkpoint to models\n",
                  "step 5700: train loss 0.0103, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 5800: train loss 0.0082, val loss 0.0289\n",
                  "saving checkpoint to models\n",
                  "step 5900: train loss 0.0092, val loss 0.0284\n",
                  "saving checkpoint to models\n",
                  "step 6000: train loss 0.0089, val loss 0.0283\n",
                  "saving checkpoint to models\n",
                  "step 6100: train loss 0.0074, val loss 0.0283\n",
                  "saving checkpoint to models\n",
                  "step 6200: train loss 0.0072, val loss 0.0285\n",
                  "saving checkpoint to models\n",
                  "step 6300: train loss 0.0083, val loss 0.0291\n",
                  "saving checkpoint to models\n",
                  "step 6400: train loss 0.0073, val loss 0.0287\n",
                  "saving checkpoint to models\n",
                  "step 6500: train loss 0.0077, val loss 0.0294\n",
                  "saving checkpoint to models\n",
                  "step 6600: train loss 0.0067, val loss 0.0285\n",
                  "saving checkpoint to models\n",
                  "step 6700: train loss 0.0075, val loss 0.0288\n",
                  "saving checkpoint to models\n",
                  "step 6800: train loss 0.0072, val loss 0.0288\n",
                  "saving checkpoint to models\n",
                  "step 6900: train loss 0.0072, val loss 0.0285\n",
                  "saving checkpoint to models\n",
                  "step 7000: train loss 0.0123, val loss 0.0325\n",
                  "saving checkpoint to models\n",
                  "step 7100: train loss 0.0055, val loss 0.0295\n",
                  "saving checkpoint to models\n",
                  "step 7200: train loss 0.0063, val loss 0.0294\n",
                  "saving checkpoint to models\n",
                  "step 7300: train loss 0.0068, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 7400: train loss 0.0056, val loss 0.0298\n",
                  "saving checkpoint to models\n",
                  "step 7500: train loss 0.0049, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 7600: train loss 0.0060, val loss 0.0291\n",
                  "saving checkpoint to models\n",
                  "step 7700: train loss 0.0061, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 7800: train loss 0.0053, val loss 0.0295\n",
                  "saving checkpoint to models\n",
                  "step 7900: train loss 0.0048, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 8000: train loss 0.0046, val loss 0.0304\n",
                  "saving checkpoint to models\n",
                  "step 8100: train loss 0.0097, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 8200: train loss 0.0047, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 8300: train loss 0.0067, val loss 0.0297\n",
                  "saving checkpoint to models\n",
                  "step 8400: train loss 0.0051, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 8500: train loss 0.0045, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 8600: train loss 0.0041, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 8700: train loss 0.0047, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 8800: train loss 0.0035, val loss 0.0295\n",
                  "saving checkpoint to models\n",
                  "step 8900: train loss 0.0038, val loss 0.0295\n",
                  "saving checkpoint to models\n",
                  "step 9000: train loss 0.0050, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 9100: train loss 0.0036, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 9200: train loss 0.0048, val loss 0.0306\n",
                  "saving checkpoint to models\n",
                  "step 9300: train loss 0.0042, val loss 0.0308\n",
                  "saving checkpoint to models\n",
                  "step 9400: train loss 0.0037, val loss 0.0303\n",
                  "saving checkpoint to models\n",
                  "step 9500: train loss 0.0044, val loss 0.0307\n",
                  "saving checkpoint to models\n",
                  "step 9600: train loss 0.0050, val loss 0.0310\n",
                  "saving checkpoint to models\n",
                  "step 9700: train loss 0.0042, val loss 0.0304\n",
                  "saving checkpoint to models\n",
                  "step 9800: train loss 0.0046, val loss 0.0296\n",
                  "saving checkpoint to models\n",
                  "step 9900: train loss 0.0032, val loss 0.0298\n",
                  "saving checkpoint to models\n",
                  "step 10000: train loss 0.0031, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 10100: train loss 0.0033, val loss 0.0297\n",
                  "saving checkpoint to models\n",
                  "step 10200: train loss 0.0029, val loss 0.0298\n",
                  "saving checkpoint to models\n",
                  "step 10300: train loss 0.0028, val loss 0.0296\n",
                  "saving checkpoint to models\n",
                  "step 10400: train loss 0.0029, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 10500: train loss 0.0028, val loss 0.0297\n",
                  "saving checkpoint to models\n",
                  "step 10600: train loss 0.0024, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 10700: train loss 0.0035, val loss 0.0295\n",
                  "saving checkpoint to models\n",
                  "step 10800: train loss 0.0026, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 10900: train loss 0.0027, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 11000: train loss 0.0029, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 11100: train loss 0.0029, val loss 0.0298\n",
                  "saving checkpoint to models\n",
                  "step 11200: train loss 0.0026, val loss 0.0298\n",
                  "saving checkpoint to models\n",
                  "step 11300: train loss 0.0023, val loss 0.0296\n",
                  "saving checkpoint to models\n",
                  "step 11400: train loss 0.0030, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 11500: train loss 0.0026, val loss 0.0296\n",
                  "saving checkpoint to models\n",
                  "step 11600: train loss 0.0028, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 11700: train loss 0.0028, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 11800: train loss 0.0026, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 11900: train loss 0.0023, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 12000: train loss 0.0026, val loss 0.0303\n",
                  "saving checkpoint to models\n",
                  "step 12100: train loss 0.0025, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 12200: train loss 0.0029, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 12300: train loss 0.0025, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 12400: train loss 0.0025, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 12500: train loss 0.0024, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 12600: train loss 0.0025, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 12700: train loss 0.0024, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 12800: train loss 0.0023, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 12900: train loss 0.0023, val loss 0.0302\n",
                  "saving checkpoint to models\n",
                  "step 13000: train loss 0.0024, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 13100: train loss 0.0023, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 13200: train loss 0.0024, val loss 0.0301\n",
                  "saving checkpoint to models\n",
                  "step 13300: train loss 0.0024, val loss 0.0298\n",
                  "saving checkpoint to models\n",
                  "step 13400: train loss 0.0024, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 13500: train loss 0.0023, val loss 0.0300\n",
                  "saving checkpoint to models\n",
                  "step 13600: train loss 0.0023, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 13700: train loss 0.0022, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 13800: train loss 0.0024, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 13900: train loss 0.0023, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "step 14000: train loss 0.0026, val loss 0.0299\n",
                  "saving checkpoint to models\n",
                  "saving final checkpoint to models\n"
               ]
            }
         ],
         "source": [
            "# Prepeare model parameters and train\n",
            "naive_approach.train(get_batch, config, model_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from animation.util import backtransform_weights\n",
            "from data.neural_field_datasets import MinMaxTransform  \n",
            "\n",
            "# Assuming the following classes and functions are defined elsewhere:\n",
            "# - INR\n",
            "# - dataset_no_transform\n",
            "# - ae_trained\n",
            "# - dataset_flatten\n",
            "\n",
            "\n",
            "\n",
            "# Configuration\n",
            "inr_kwargs = {\"n_layers\": 3, \"in_dim\": 2, \"up_scale\": 16}\n",
            "image_size = (28, 28)\n",
            "idx = 0\n",
            "# Get dataset elements\n",
            "original_dict = dataset_no_transform[idx][0]\n",
            "\n",
            "model = RegressionTransformer(model_config)\n",
            "model.load_state_dict(torch.load(\"./models/final_model.pth\")[\"model\"])\n",
            "\n",
            "\n",
            "sample = dataset[idx][0]\n",
            "X, Y = (sample[:model_config.block_size].unsqueeze(-1).unsqueeze(0), sample[1: 1 + model_config.block_size].unsqueeze(-1).unsqueeze(0))\n",
            "\n",
            "# autoregressive process\n",
            "seq = torch.zeros((1, 593, 1))\n",
            "seq[0][0][0] = X[0][0][0]\n",
            "\n",
            "for i in range(0, model_config.block_size):\n",
            "    pred, _loss = model(seq[:, :-1], Y)\n",
            "    seq[0][i + 1][0] = pred[0][i][0]\n",
            "\n",
            "\n",
            "#pred, loss = model(X, Y)\n",
            "#pred = torch.cat([X[0][0], pred[0].squeeze(-1)]).unsqueeze(0).unsqueeze(-1)\n",
            "\n",
            "\n",
            "minmax_transformer = MinMaxTransform(min_value=min_ours, max_value=max_ours)\n",
            "dataset_ele_flattened = minmax_transformer.reverse(seq[0]).unsqueeze(0)\n",
            "\n",
            "# Backtransform weights\n",
            "reconstructed_dict = backtransform_weights(dataset_ele_flattened, original_dict)\n",
            "\n",
            "from animation.util import reconstruct_image\n",
            "from networks.mlp_models import MLP3D\n",
            "\n",
            "\n",
            "model_config_nef = {\n",
            "        \"out_size\": 1,\n",
            "        \"hidden_neurons\": [16, 16],\n",
            "        \"use_leaky_relu\": False,\n",
            "        \"output_type\": \"logits\",\n",
            "        \"input_dims\": 2,\n",
            "        \"multires\": 4,\n",
            "    }\n",
            "\n",
            "model = MLP3D(**model_config_nef)\n",
            "model.load_state_dict(reconstructed_dict)\n",
            "reconstructed_tensor = reconstruct_image(model)\n",
            "model.load_state_dict(original_dict)\n",
            "ground_truth_tensor = reconstruct_image(model)\n",
            "\n",
            "# Plotting the tensors as heatmaps in grayscale\n",
            "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
            "\n",
            "axes[0].imshow(ground_truth_tensor, cmap='gray', aspect='auto')\n",
            "axes[0].set_title('Ground Truth')\n",
            "axes[0].set_xlabel('X-axis')\n",
            "axes[0].set_ylabel('Y-axis')\n",
            "\n",
            "axes[1].imshow(reconstructed_tensor, cmap='gray', aspect='auto')\n",
            "axes[1].set_title('Reconstructed')\n",
            "axes[1].set_xlabel('X-axis')\n",
            "axes[1].set_ylabel('Y-axis')\n",
            "\n",
            "plt.colorbar(axes[0].imshow(ground_truth_tensor, cmap='gray', aspect='auto'), ax=axes[0])\n",
            "plt.colorbar(axes[1].imshow(reconstructed_tensor, cmap='gray', aspect='auto'), ax=axes[1])\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
