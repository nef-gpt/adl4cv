{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luis/uni/adl4cv/adl4cv/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from data.DWSNets_dataset import DWSNetsDataset, LayerOneHotTransform, FlattenTransform, BiasFlagTransform\n",
    "from networks.naive_rq_ae import RQAutoencoder, RQAutoencoderConfig\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(dir_path, \"adl4cv\", \"datasets\", \"DWSNets\", \"mnist-inrs\")\n",
    "\n",
    "class AutoencoderTransform(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bias = BiasFlagTransform()\n",
    "    self.flatten = FlattenTransform()\n",
    "    self.layer_one_hot = LayerOneHotTransform()\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    bias, _ = self.bias(x, y)\n",
    "    layer, _ = self.layer_one_hot(x, y)    \n",
    "    x, _ = self.flatten(x, y)\n",
    "    return torch.hstack((x, layer, bias)), y\n",
    "\n",
    "\n",
    "dataset_no_transform = DWSNetsDataset(data_root)\n",
    "train_dataset = DWSNetsDataset(data_root, transform=AutoencoderTransform())\n",
    "test_dataset = DWSNetsDataset(data_root, split=\"test\", transform=AutoencoderTransform())\n",
    "\n",
    "path = \"datasets/DWSNets/mnist-inrs/mnist_splits.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0784,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0276,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0030,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0686,  0.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.0870,  0.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [-0.1302,  0.0000,  0.0000,  1.0000,  1.0000]])\n",
      "tensor([[ 0.3800, -0.2634,  0.4970,  0.1051,  0.3101],\n",
      "        [ 0.3809, -0.2645,  0.4972,  0.1039,  0.3103],\n",
      "        [ 0.3808, -0.2642,  0.4972,  0.1042,  0.3102],\n",
      "        ...,\n",
      "        [ 0.3828, -0.2663,  0.4978,  0.1019,  0.3105],\n",
      "        [ 0.3844, -0.2680,  0.4983,  0.1000,  0.3107],\n",
      "        [ 0.3863, -0.2706,  0.4971,  0.0986,  0.3117]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ae_config = RQAutoencoderConfig(dim_l=(5, 5, 5))\n",
    "ae = RQAutoencoder(ae_config)\n",
    "\n",
    "print(train_dataset[203][0])\n",
    "print(ae(train_dataset[203][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0041,  1.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].flatten().reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1185, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluis-muschal\u001b[0m (\u001b[33madl-for-cv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/luis/uni/adl4cv/adl4cv/wandb/run-20240523_172735-q9rkplmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adl-for-cv/autoencoder/runs/q9rkplmw' target=\"_blank\">run-2024-05-23-17-27-34</a></strong> to <a href='https://wandb.ai/adl-for-cv/autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adl-for-cv/autoencoder' target=\"_blank\">https://wandb.ai/adl-for-cv/autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adl-for-cv/autoencoder/runs/q9rkplmw' target=\"_blank\">https://wandb.ai/adl-for-cv/autoencoder/runs/q9rkplmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▇▆▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▂▃▅▆██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>1</td></tr><tr><td>epoch</td><td>4999</td></tr><tr><td>loss</td><td>0.00014</td></tr><tr><td>lr</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run-2024-05-23-17-27-34</strong> at: <a href='https://wandb.ai/adl-for-cv/autoencoder/runs/q9rkplmw' target=\"_blank\">https://wandb.ai/adl-for-cv/autoencoder/runs/q9rkplmw</a><br/> View project at: <a href='https://wandb.ai/adl-for-cv/autoencoder' target=\"_blank\">https://wandb.ai/adl-for-cv/autoencoder</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240523_172735-q9rkplmw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from training.training_autoencoder import train_model, TrainingConfig\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader([train_dataset[0]], batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader([test_dataset[0]], batch_size=1, shuffle=True)\n",
    "\n",
    "train_config = TrainingConfig()\n",
    "train_config.max_iters = 5000\n",
    "train_config.always_save_checkpoint = True\n",
    "train_config.weight_decay = 0.0\n",
    "train_config.learning_rate = 1e-3\n",
    "train_config.lr_decay_iters = 5000\n",
    "train_config.log_interval = 1\n",
    "\n",
    "class AutoencoderLoss(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.mse = MSELoss()\n",
    "    self.ce = CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, pred, true):\n",
    "    # take first column of pred and true and compare them with mse\n",
    "    x_pred = pred[:, :1]\n",
    "    x_true = true[:, :1]\n",
    "    mse_loss = self.mse(x_pred, x_true)\n",
    "\n",
    "    # compare one hot encoding (eg feature 1, 2, 3) with cross entropy loss\n",
    "    layer_pred = pred[:, 1:4]\n",
    "    layer_true = true[:, 1:4]\n",
    "    ce_loss = self.ce(layer_pred, layer_true)\n",
    "\n",
    "    # compare bias flag\n",
    "    bias_pred = pred[:, 4]\n",
    "    bias_true = true[:, 4]\n",
    "    bias_loss = self.ce(bias_pred, bias_true)\n",
    "\n",
    "    return mse_loss + ce_loss + bias_loss\n",
    "\n",
    "train_model(train_config, ae_config, train_dataloader, test_dataloader, MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"./models/model_epoch_0.pth\"\n",
    "ae_trained = RQAutoencoder(ae_config)\n",
    "ae_trained.load_state_dict(torch.load(PATH)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0041,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0303,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0380,  1.0000,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1311,  0.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.1318,  0.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [-0.1846,  0.0000,  0.0000,  1.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2182,  1.8510, -0.2103, -0.2100, -0.2099],\n",
      "        [-0.2182,  1.8485, -0.2101, -0.2091, -0.2099],\n",
      "        [-0.2182,  1.8479, -0.2100, -0.2089, -0.2099],\n",
      "        ...,\n",
      "        [-0.2441,  1.6777, -0.2271, -0.1919, -0.1876],\n",
      "        [-0.2441,  1.6609, -0.2250, -0.1852, -0.1880],\n",
      "        [-0.2556,  1.6280, -0.2380, -0.1948, -0.1770]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(ae_trained(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0007, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSELoss()\n",
    "idx = 0\n",
    "loss(ae_trained(train_dataset[idx][0]), ae(train_dataset[idx][0]))/len(ae_trained(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1976,  1.8893, -0.2167, -0.2187, -0.2080],\n",
       "        [-0.1976,  1.8893, -0.2167, -0.2187, -0.2080],\n",
       "        [-0.1976,  1.8893, -0.2167, -0.2187, -0.2080],\n",
       "        ...,\n",
       "        [-0.1402,  1.7051, -0.1938, -0.1777, -0.2014],\n",
       "        [-0.1720,  1.7908, -0.2006, -0.2040, -0.2080],\n",
       "        [-0.1041,  1.5429, -0.1611, -0.1606, -0.2079]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, indices, commit_loss = ae_trained.encode_to_cb(train_dataset[idx][0])\n",
    "ae_trained.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 2]' is invalid for input of size 320",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_ele \u001b[38;5;241m=\u001b[39m dataset_no_transform[idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m dataset_ele_flattened\u001b[38;5;241m=\u001b[39m ae_trained(train_dataset[idx][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m reconstructed_dict \u001b[38;5;241m=\u001b[39m \u001b[43mbacktransform_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_ele_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_ele\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m ae_trained(train_dataset[idx][\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[71], line 10\u001b[0m, in \u001b[0;36mbacktransform_weights\u001b[0;34m(flattened_weights, original_weights_dict)\u001b[0m\n\u001b[1;32m      8\u001b[0m flattened_slice \u001b[38;5;241m=\u001b[39m flattened_weights[start:start \u001b[38;5;241m+\u001b[39m num_elements]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Reshape the slice to the shape of the original tensor\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m reconstructed_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mflattened_slice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add to the reconstructed dictionary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m reconstructed_dict[key] \u001b[38;5;241m=\u001b[39m reconstructed_tensor\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 2]' is invalid for input of size 320"
     ]
    }
   ],
   "source": [
    "def backtransform_weights(flattened_weights, original_weights_dict):\n",
    "    reconstructed_dict = OrderedDict()\n",
    "    start = 0\n",
    "    for key, tensor in original_weights_dict.items():\n",
    "        # Get the number of elements in the tensor\n",
    "        num_elements = tensor.numel()\n",
    "        # Get the slice of the flattened weights corresponding to this tensor\n",
    "        flattened_slice = flattened_weights[start:start + num_elements]\n",
    "        # Reshape the slice to the shape of the original tensor\n",
    "        reconstructed_tensor = flattened_slice.view(tensor.shape)\n",
    "        # Add to the reconstructed dictionary\n",
    "        reconstructed_dict[key] = reconstructed_tensor\n",
    "        # Update the start index for the next slice\n",
    "        start += num_elements\n",
    "    \n",
    "    return reconstructed_dict\n",
    "\n",
    "\n",
    "idx = 1\n",
    "dataset_ele = dataset_no_transform[idx][0]\n",
    "dataset_ele_flattened= ae_trained(train_dataset[idx][0])[:, 0]\n",
    "\n",
    "reconstructed_dict = backtransform_weights(dataset_ele_flattened, dataset_ele)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
