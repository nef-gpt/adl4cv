{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
                  "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca-fanselau\u001b[0m (\u001b[33madl-for-cv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "False"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import torch\n",
            "from vector_quantize_pytorch import VectorQuantize\n",
            "import os\n",
            "from data.neural_field_datasets import MnistNeFDataset, TokenTransform\n",
            "from training import training_nano_gpt\n",
            "\n",
            "from networks.nano_gpt import GPTConfig\n",
            "\n",
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "ename": "FileNotFoundError",
               "evalue": "[Errno 2] No such file or directory: '/Users/luca/uni/master/second-semester/adl4cv/models/vqs/vq_mnist_with_all_5_conditioned_n_501.pt'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m data_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madl4cv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# load used vector quantizer\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m vq_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvqs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvq_mnist_with_all_5_conditioned_n_501.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m vq \u001b[38;5;241m=\u001b[39m VectorQuantize(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvq_dicts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvq_config\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m vq\u001b[38;5;241m.\u001b[39mload_state_dict(vq_dicts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
                  "File \u001b[0;32m~/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
                  "File \u001b[0;32m~/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
                  "File \u001b[0;32m~/uni/master/second-semester/adl4cv/.venv/lib/python3.12/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
                  "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/luca/uni/master/second-semester/adl4cv/models/vqs/vq_mnist_with_all_5_conditioned_n_501.pt'"
               ]
            }
         ],
         "source": [
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
            "data_root = os.path.join(dir_path, \"adl4cv\")\n",
            "\n",
            "# load used vector quantizer\n",
            "vq_dicts = torch.load(os.path.join(data_root, \"models\", \"vqs\", \"vq_mnist_with_all_5_conditioned_n_501.pt\"))\n",
            "vq = VectorQuantize(**vq_dicts[\"vq_config\"])\n",
            "vq.load_state_dict(vq_dicts[\"state_dict\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "60000"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "len(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "config = training_nano_gpt.Config()\n",
            "config.learning_rate=2e-3\n",
            "config.max_iters = 25000\n",
            "config.weight_decay=0.0\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=config.max_iters\n",
            "config.warmup_iters=0.05*config.max_iters\n",
            "config.batch_size = 32\n",
            "config.gradient_accumulation_steps = 1\n",
            "config.init_from = \"scratch\"\n",
            "config.out_dir =\"models/token_transformer\"\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "config.eval_interval = 250\n",
            "config.metric_interval = 250\n",
            "\n",
            "model_config = GPTConfig(n_embd=108, block_size=len(dataset[0][0]), n_head=12, n_layer=6, vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 11, dropout=0.0)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "#early_stopping = training_nano_gpt.EarlyStopper(20)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'model_config = GPTConfig(\\n    n_embd=120, \\n    block_size=len(dataset[0][0]), \\n    n_head=12, n_layer=6, \\n    vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1,\\n    dropout=0.0\\n    )'"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"\"\"model_config = GPTConfig(\n",
            "    n_embd=120, \n",
            "    block_size=len(dataset[0][0]), \n",
            "    n_head=12, n_layer=6, \n",
            "    vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1,\n",
            "    dropout=0.0\n",
            "    )\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "cb_size = vq_dicts[\"vq_config\"][\"codebook_size\"]\n",
            "token_dict = {\n",
            "    \"SOS\": cb_size - 11,\n",
            "    \"0\": cb_size - 10,\n",
            "    \"1\": cb_size - 9,\n",
            "    \"2\": cb_size - 8,\n",
            "    \"3\": cb_size - 7,\n",
            "    \"4\": cb_size - 6,\n",
            "    \"5\": cb_size - 5,\n",
            "    \"6\": cb_size - 4,\n",
            "    \"7\": cb_size - 3,\n",
            "    \"8\": cb_size - 2,\n",
            "    \"9\": cb_size - 1\n",
            "}\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Where to put?\n",
            "# Maybe adjust dataset to be able to work with splitting data and then rewrite TokenTransform \n",
            "# to do the job combined with pytorch dataloader (get_batch == __call__ of Dataloader)\n",
            "\n",
            "def create_split_indices(n, train_ratio=0.9):\n",
            "    # Generate a random permutation of indices from 0 to n-1\n",
            "    shuffled_indices = torch.randperm(n)\n",
            "    # Determine the cut-off for training data\n",
            "    train_size = int(train_ratio * n)\n",
            "    # Split indices into training and validation sets\n",
            "    train_indices = shuffled_indices[:train_size]\n",
            "    val_indices = shuffled_indices[train_size:]\n",
            "    return train_indices, val_indices\n",
            "\n",
            "train_indices, val_indices = create_split_indices(len(dataset))\n",
            "\n",
            "def get_batch_lambda(config, dataset, model_config, split):\n",
            "    batch_size = config.batch_size\n",
            "    \n",
            "\n",
            "    # Select indices based on the split\n",
            "    if split == 'train':\n",
            "        # Randomly select batch_size indices from the train_indices\n",
            "        indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
            "    elif split == 'val':\n",
            "        # Randomly select batch_size indices from the val_indices\n",
            "        indices = val_indices[torch.randint(0, len(val_indices), (batch_size,))]\n",
            "    \n",
            "    \n",
            "    # Initialize lists to hold the sequences and labels\n",
            "    samples = []\n",
            "    labels = []\n",
            "\n",
            "    # Collect samples and labels\n",
            "    for idx in indices:\n",
            "        sample, label = dataset[idx]\n",
            "        start_tokens = torch.Tensor([token_dict[\"SOS\"], token_dict[str(label)]]).long()  # Start of sequence token\n",
            "        sample = torch.cat((start_tokens, sample), dim=0)\n",
            "        #start_tokens = torch.Tensor([0]).long()  # Start of sequence token\n",
            "        #sample = torch.cat((start_tokens, sample + 1), dim=0)\n",
            "        samples.append(sample)\n",
            "        labels.append(label)\n",
            "\n",
            "    # Prepare the sequences for model input\n",
            "    max_len = samples[0].size(0)\n",
            "    x = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    y = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    \n",
            "    for i, sample in enumerate(samples):\n",
            "        end_index = sample.size(0) - 1\n",
            "        x[i, :end_index] = sample[:-1]  # Exclude the last token for x\n",
            "        y[i, :end_index] = sample[1:]   # Exclude the first token for y\n",
            "\n",
            "    # Ensure x and y are the correct shape (batch_size, block_size) if needed:\n",
            "    # Here, we truncate to `block_size` if samples are longer than `block_size`.\n",
            "    x = x[:, :model_config.block_size]\n",
            "    y = y[:, :model_config.block_size]\n",
            "\n",
            "    # x and y have to be\n",
            "    x = x.to(config.device)\n",
            "    y = y.to(config.device)\n",
            "\n",
            "    return x, y\n",
            "\n",
            "create_get_batch = lambda config, dataset, model_config: lambda split: get_batch_lambda(config, dataset, model_config, split)\n",
            "get_batch = create_get_batch(config, dataset, model_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "30294000"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "len(train_indices)*dataset[0][0].shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Initializing a new model from scratch\n",
                  "number of parameters: 0.88M\n",
                  "num decayed parameter tensors: 26, with 928,044 parameters\n",
                  "num non-decayed parameter tensors: 50, with 8,640 parameters\n",
                  "using fused AdamW: True\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "wandb version 0.17.2 is available!  To upgrade, please run:\n",
                     " $ pip install wandb --upgrade"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Tracking run with wandb version 0.16.6"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Run data is saved locally in <code>/home/luca/uni/master/adl4cv/wandb/run-20240617_215013-o1i4an7j</code>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Syncing run <strong><a href='https://wandb.ai/adl-for-cv/naive_token_transformer/runs/o1i4an7j' target=\"_blank\">run-2024-06-17-21-50-11</a></strong> to <a href='https://wandb.ai/adl-for-cv/naive_token_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     " View project at <a href='https://wandb.ai/adl-for-cv/naive_token_transformer' target=\"_blank\">https://wandb.ai/adl-for-cv/naive_token_transformer</a>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     " View run at <a href='https://wandb.ai/adl-for-cv/naive_token_transformer/runs/o1i4an7j' target=\"_blank\">https://wandb.ai/adl-for-cv/naive_token_transformer/runs/o1i4an7j</a>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 0: train loss 5.5711, val loss 5.5705\n",
                  "step 250: train loss 3.6609, val loss 3.6622\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 500: train loss 3.5599, val loss 3.5406\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 750: train loss 3.4912, val loss 3.4730\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1000: train loss 3.3984, val loss 3.3828\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1250: train loss 3.2897, val loss 3.2931\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1500: train loss 3.1931, val loss 3.1998\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1750: train loss 3.1084, val loss 3.0913\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2000: train loss 3.0090, val loss 3.0086\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2250: train loss 2.9318, val loss 2.9448\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2500: train loss 2.8639, val loss 2.8792\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2750: train loss 2.8279, val loss 2.8158\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3000: train loss 2.7990, val loss 2.8270\n",
                  "step 3250: train loss 2.7862, val loss 2.7954\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3500: train loss 2.7615, val loss 2.7482\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3750: train loss 2.7274, val loss 2.7182\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 4000: train loss 2.7219, val loss 2.7132\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 4250: train loss 2.6980, val loss 2.6948\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 4500: train loss 2.6726, val loss 2.7065\n",
                  "step 4750: train loss 2.6686, val loss 2.6647\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 5000: train loss 2.6714, val loss 2.6727\n",
                  "step 5250: train loss 2.6452, val loss 2.6653\n",
                  "step 5500: train loss 2.6558, val loss 2.6488\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 5750: train loss 2.6206, val loss 2.6285\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 6000: train loss 2.6277, val loss 2.6497\n",
                  "step 6250: train loss 2.6111, val loss 2.6150\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 6500: train loss 2.6054, val loss 2.6206\n",
                  "step 6750: train loss 2.5924, val loss 2.6260\n",
                  "step 7000: train loss 2.5895, val loss 2.5916\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 7250: train loss 2.6029, val loss 2.5904\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 7500: train loss 2.6001, val loss 2.6029\n",
                  "step 7750: train loss 2.5632, val loss 2.5754\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 8000: train loss 2.5825, val loss 2.5696\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 8250: train loss 2.5585, val loss 2.5601\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 8500: train loss 2.5838, val loss 2.5697\n",
                  "step 8750: train loss 2.5509, val loss 2.5751\n",
                  "step 9000: train loss 2.5510, val loss 2.5553\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 9250: train loss 2.5668, val loss 2.5674\n",
                  "step 9500: train loss 2.5488, val loss 2.5577\n",
                  "step 9750: train loss 2.5426, val loss 2.5463\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 10000: train loss 2.5475, val loss 2.5432\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 10250: train loss 2.5352, val loss 2.5308\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 10500: train loss 2.5297, val loss 2.5318\n",
                  "step 10750: train loss 2.5309, val loss 2.5376\n",
                  "step 11000: train loss 2.5079, val loss 2.5344\n",
                  "step 11250: train loss 2.5274, val loss 2.5161\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 11500: train loss 2.5184, val loss 2.5328\n",
                  "step 11750: train loss 2.4993, val loss 2.5505\n",
                  "step 12000: train loss 2.5109, val loss 2.5047\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 12250: train loss 2.5414, val loss 2.5181\n",
                  "step 12500: train loss 2.5052, val loss 2.5241\n",
                  "step 12750: train loss 2.4873, val loss 2.5031\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 13000: train loss 2.4701, val loss 2.4811\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 13250: train loss 2.5043, val loss 2.4942\n",
                  "step 13500: train loss 2.4796, val loss 2.4842\n",
                  "step 13750: train loss 2.4731, val loss 2.5096\n",
                  "step 14000: train loss 2.4700, val loss 2.4990\n",
                  "step 14250: train loss 2.4719, val loss 2.4813\n",
                  "step 14500: train loss 2.4715, val loss 2.4808\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 14750: train loss 2.4566, val loss 2.4874\n",
                  "step 15000: train loss 2.4874, val loss 2.4755\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15250: train loss 2.4469, val loss 2.4536\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15500: train loss 2.4673, val loss 2.4680\n",
                  "step 15750: train loss 2.4520, val loss 2.4674\n",
                  "step 16000: train loss 2.4700, val loss 2.4497\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 16250: train loss 2.4351, val loss 2.4575\n",
                  "step 16500: train loss 2.4462, val loss 2.4638\n",
                  "step 16750: train loss 2.4313, val loss 2.4485\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 17000: train loss 2.4397, val loss 2.4369\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 17250: train loss 2.4131, val loss 2.4524\n",
                  "step 17500: train loss 2.4228, val loss 2.4241\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 17750: train loss 2.4411, val loss 2.4463\n",
                  "step 18000: train loss 2.4131, val loss 2.4370\n",
                  "step 18250: train loss 2.4061, val loss 2.3962\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 18500: train loss 2.4049, val loss 2.4368\n",
                  "step 18750: train loss 2.4180, val loss 2.4098\n",
                  "step 19000: train loss 2.3940, val loss 2.3968\n",
                  "step 19250: train loss 2.4116, val loss 2.4210\n",
                  "step 19500: train loss 2.3887, val loss 2.4372\n",
                  "step 19750: train loss 2.3980, val loss 2.3834\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 20000: train loss 2.3882, val loss 2.4182\n",
                  "step 20250: train loss 2.3853, val loss 2.4215\n",
                  "step 20500: train loss 2.3807, val loss 2.3874\n",
                  "step 20750: train loss 2.3776, val loss 2.4298\n",
                  "step 21000: train loss 2.3936, val loss 2.4076\n",
                  "step 21250: train loss 2.3939, val loss 2.4055\n",
                  "step 21500: train loss 2.3672, val loss 2.4182\n",
                  "step 21750: train loss 2.3742, val loss 2.3686\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 22000: train loss 2.3784, val loss 2.3613\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 22250: train loss 2.3817, val loss 2.3823\n",
                  "step 22500: train loss 2.3651, val loss 2.3879\n",
                  "step 22750: train loss 2.3732, val loss 2.3815\n",
                  "step 23000: train loss 2.3871, val loss 2.3789\n",
                  "step 23250: train loss 2.3778, val loss 2.3893\n",
                  "step 23500: train loss 2.3441, val loss 2.4208\n",
                  "step 23750: train loss 2.3900, val loss 2.3881\n",
                  "step 24000: train loss 2.3814, val loss 2.3873\n",
                  "step 24250: train loss 2.3543, val loss 2.3687\n",
                  "step 24500: train loss 2.3495, val loss 2.3926\n",
                  "step 24750: train loss 2.3551, val loss 2.3930\n",
                  "step 25000: train loss 2.3492, val loss 2.3962\n"
               ]
            }
         ],
         "source": [
            "# Prepeare model parameters and train\n",
            "import wandb\n",
            "wandb.login()\n",
            "trained_model = training_nano_gpt.train(get_batch, config, model_config, vq, vq_dicts[\"vq_config\"], token_dict=token_dict)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "number of parameters: 0.88M\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([[False, False, False, False, False, False,  True, False, False, False,\n",
                     "         False, False, False, False, False,  True, False, False, False, False,\n",
                     "          True, False, False, False, False, False,  True,  True, False, False,\n",
                     "         False, False, False, False, False,  True, False, False, False, False,\n",
                     "         False, False, False, False, False,  True,  True, False, False,  True,\n",
                     "         False, False, False,  True, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False,  True,  True,  True,\n",
                     "         False, False, False,  True, False, False, False, False,  True, False,\n",
                     "         False, False, False, False,  True, False, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False, False,  True, False,\n",
                     "         False,  True, False,  True,  True, False, False,  True, False, False,\n",
                     "         False, False,  True, False, False, False, False,  True,  True, False,\n",
                     "          True, False, False,  True,  True,  True, False, False, False, False,\n",
                     "         False, False, False, False, False, False,  True, False,  True, False,\n",
                     "         False, False,  True, False,  True, False, False, False,  True, False,\n",
                     "         False,  True, False, False,  True,  True, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False,  True,\n",
                     "          True, False,  True, False, False, False, False, False, False, False,\n",
                     "          True, False, False, False,  True, False,  True, False, False, False,\n",
                     "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
                     "         False,  True, False, False, False, False,  True, False, False, False,\n",
                     "          True, False, False, False, False, False, False, False,  True,  True,\n",
                     "          True,  True, False, False, False, False, False,  True, False, False,\n",
                     "         False,  True, False, False, False,  True, False, False, False, False,\n",
                     "          True, False,  True, False, False, False, False, False,  True, False,\n",
                     "          True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
                     "          True,  True,  True,  True, False, False, False, False,  True, False,\n",
                     "         False, False, False, False, False,  True, False, False, False,  True,\n",
                     "         False,  True,  True, False,  True, False, False, False,  True, False,\n",
                     "         False,  True, False,  True, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False,  True, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False,  True,  True,  True, False, False,  True, False,\n",
                     "         False, False, False, False, False, False, False, False,  True,  True,\n",
                     "          True, False, False, False, False, False, False, False, False, False,\n",
                     "          True, False, False, False, False,  True,  True, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False,  True,\n",
                     "         False,  True, False, False, False, False, False,  True, False, False,\n",
                     "         False, False, False, False, False, False, False,  True, False, False,\n",
                     "         False,  True,  True,  True, False,  True,  True, False,  True,  True,\n",
                     "          True,  True,  True,  True, False,  True, False, False,  True, False,\n",
                     "         False, False, False, False, False,  True, False,  True, False,  True,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False, False, False,  True,\n",
                     "          True,  True,  True, False,  True,  True, False, False, False,  True,\n",
                     "          True,  True,  True, False,  True, False, False,  True, False, False,\n",
                     "         False, False, False, False, False, False, False,  True, False, False,\n",
                     "         False, False, False, False,  True,  True, False, False, False, False,\n",
                     "         False,  True, False,  True,  True, False, False, False, False, False,\n",
                     "         False, False, False,  True, False,  True, False, False, False,  True,\n",
                     "         False, False, False,  True, False, False,  True, False, False, False,\n",
                     "          True, False, False, False,  True,  True, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False,  True, False, False,\n",
                     "          True, False, False, False, False,  True, False,  True,  True, False,\n",
                     "          True,  True, False,  True, False, False,  True, False, False,  True,\n",
                     "          True]], device='cuda:0')"
                  ]
               },
               "execution_count": 16,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from networks.nano_gpt import GPT\n",
            "from utils import get_default_device\n",
            "\n",
            "model_dict = torch.load(\"./models/token_transformer/ckpt.pt\")\n",
            "# Configuration\n",
            "idx = 3\n",
            "\n",
            "device = get_default_device()\n",
            "model = GPT(model_config)#model_dict[\"model_args\"])\n",
            "model.to(device=device)\n",
            "model.load_state_dict(model_dict[\"model\"])\n",
            "model.eval()\n",
            "\n",
            "vq = VectorQuantize(**model_dict[\"vq_config\"])\n",
            "vq.load_state_dict(model_dict[\"vq_state_dict\"])\n",
            "vq.eval()\n",
            "\n",
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "\n",
            "\n",
            "sample = dataset[0][0]\n",
            "X, Y = get_batch('val')\n",
            "X, Y = (X[0].unsqueeze(0), Y[0].unsqueeze(0))\n",
            "pred, _ = model(X, Y)\n",
            "# Sanity Check\n",
            "# Should be all true except first/second element\n",
            "pred.argmax(dim=-1)==Y\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor(6, device='cuda:0')\n",
                  "tensor(6, device='cuda:0')\n",
                  "tensor(2, device='cuda:0')\n",
                  "tensor(5, device='cuda:0')\n"
               ]
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 2000x1000 with 4 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "from animation.util import backtransform_weights, reconstruct_image\n",
            "from networks.mlp_models import MLP3D\n",
            "\n",
            "ij_len = 2\n",
            "# Plotting the tensors as heatmaps in grayscale\n",
            "fig, axes = plt.subplots(ij_len, ij_len, figsize=(20, 10))\n",
            "\n",
            "SOS = torch.Tensor([[0]]).long().to(device)\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "for i in range(ij_len):\n",
            "    for j in range(ij_len):\n",
            "\n",
            "        model.eval()\n",
            "        novel_tokens = model.generate(SOS, dataset[0][0].shape[0] + 1, temperature=1.0, top_k=None)[:, 1:]\n",
            "\n",
            "        print(novel_tokens[0][0])\n",
            "        novel_tokens = novel_tokens[:, 1:].unsqueeze(-1).to(\"cpu\")\n",
            "                                                                                                                         \n",
            "\n",
            "        max_similarity = 0\n",
            "        \"\"\"\n",
            "        for data in dataset:\n",
            "            similarity = (data[0].to(device)==novel_tokens.squeeze(-1).squeeze(0).to(device)).int().sum()\n",
            "            if similarity > max_similarity:\n",
            "                max_similarity = similarity\n",
            "        \"\"\"\n",
            "        #print(f\"Maximum Similarity of picture (i, j) {(i, j)}: {max_similarity}\")\n",
            "\n",
            "        novel_weights= vq.get_codes_from_indices((novel_tokens-11))\n",
            "\n",
            "        dataset_no_transform = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), **kwargs)\n",
            "        original_dict = dataset_no_transform[0][0]\n",
            "\n",
            "        reconstructed_dict = backtransform_weights(novel_weights, original_dict[\"state_dict\"])\n",
            "\n",
            "        mlp3d = MLP3D(**original_dict[\"model_config\"])\n",
            "        mlp3d.load_state_dict(reconstructed_dict)\n",
            "        reconstructed_tensor = reconstruct_image(mlp3d)\n",
            "\n",
            "        axes[i][j].imshow(reconstructed_tensor, cmap='gray', aspect='auto')\n",
            "\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
