{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/luca/.cache/pypoetry/virtualenvs/adl4cv-OvNqwVNf-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n",
                  "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
                  "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca-fanselau\u001b[0m (\u001b[33madl-for-cv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import torch\n",
            "from vector_quantize_pytorch import VectorQuantize\n",
            "import os\n",
            "from data.neural_field_datasets import MnistNeFDataset, TokenTransform\n",
            "from training import training_nano_gpt\n",
            "\n",
            "from networks.nano_gpt import GPTConfig\n",
            "\n",
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<All keys matched successfully>"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
            "data_root = os.path.join(dir_path, \"adl4cv\")\n",
            "\n",
            "# load used vector quantizer\n",
            "vq_dicts = torch.load(os.path.join(data_root, \"models\", \"vqs\", \"vq_mnist_with_all_5_conditioned_n_501.pt\"))\n",
            "vq = VectorQuantize(**vq_dicts[\"vq_config\"])\n",
            "vq.load_state_dict(vq_dicts[\"state_dict\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "60000"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "len(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "config = training_nano_gpt.Config()\n",
            "config.learning_rate=4e-3\n",
            "config.max_iters = 20000\n",
            "config.weight_decay=0.0\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=config.max_iters\n",
            "config.warmup_iters=0.05*config.max_iters\n",
            "config.batch_size = 64\n",
            "config.gradient_accumulation_steps = 1\n",
            "config.init_from = \"resume\"\n",
            "config.out_dir =\"models/token_transformer\"\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "config.eval_interval = 250\n",
            "\n",
            "model_config = GPTConfig(n_embd=108, block_size=len(dataset[0][0]), n_head=12, n_layer=6, vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1, dropout=0.0)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "#early_stopping = training_nano_gpt.EarlyStopper(20)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'model_config = GPTConfig(\\n    n_embd=120, \\n    block_size=len(dataset[0][0]), \\n    n_head=12, n_layer=6, \\n    vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1,\\n    dropout=0.0\\n    )'"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"\"\"model_config = GPTConfig(\n",
            "    n_embd=120, \n",
            "    block_size=len(dataset[0][0]), \n",
            "    n_head=12, n_layer=6, \n",
            "    vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1,\n",
            "    dropout=0.0\n",
            "    )\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "token_dict = {\n",
            "    \"SOS\": 0,\n",
            "    \"0\": 0,\n",
            "    \"1\": 1,\n",
            "    \"2\": 2,\n",
            "    \"3\": 3,\n",
            "    \"4\": 4,\n",
            "    \"5\": 5,\n",
            "    \"6\": 6,\n",
            "    \"7\": 7,\n",
            "    \"8\": 8,\n",
            "    \"9\": 9\n",
            "}\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Where to put?\n",
            "# Maybe adjust dataset to be able to work with splitting data and then rewrite TokenTransform \n",
            "# to do the job combined with pytorch dataloader (get_batch == __call__ of Dataloader)\n",
            "\n",
            "def create_split_indices(n, train_ratio=0.9):\n",
            "    # Generate a random permutation of indices from 0 to n-1\n",
            "    shuffled_indices = torch.randperm(n)\n",
            "    # Determine the cut-off for training data\n",
            "    train_size = int(train_ratio * n)\n",
            "    # Split indices into training and validation sets\n",
            "    train_indices = shuffled_indices[:train_size]\n",
            "    val_indices = shuffled_indices[train_size:]\n",
            "    return train_indices, val_indices\n",
            "\n",
            "train_indices, val_indices = create_split_indices(len(dataset))\n",
            "\n",
            "def get_batch_lambda(config, dataset, model_config, split):\n",
            "    batch_size = config.batch_size\n",
            "    \n",
            "\n",
            "    # Select indices based on the split\n",
            "    if split == 'train':\n",
            "        # Randomly select batch_size indices from the train_indices\n",
            "        indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
            "    elif split == 'val':\n",
            "        # Randomly select batch_size indices from the val_indices\n",
            "        indices = val_indices[torch.randint(0, len(val_indices), (batch_size,))]\n",
            "    \n",
            "    \n",
            "    # Initialize lists to hold the sequences and labels\n",
            "    samples = []\n",
            "    labels = []\n",
            "\n",
            "    # Collect samples and labels\n",
            "    for idx in indices:\n",
            "        sample, label = dataset[idx]\n",
            "        start_tokens = torch.Tensor([token_dict[\"SOS\"], token_dict[str(label)]]).long()  # Start of sequence token\n",
            "        sample = torch.cat((start_tokens, len(token_dict.keys()) + sample), dim=0)\n",
            "        #start_tokens = torch.Tensor([0]).long()  # Start of sequence token\n",
            "        #sample = torch.cat((start_tokens, sample + 1), dim=0)\n",
            "        samples.append(sample)\n",
            "        labels.append(label)\n",
            "\n",
            "    # Prepare the sequences for model input\n",
            "    max_len = samples[0].size(0)\n",
            "    x = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    y = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    \n",
            "    for i, sample in enumerate(samples):\n",
            "        end_index = sample.size(0) - 1\n",
            "        x[i, :end_index] = sample[:-1]  # Exclude the last token for x\n",
            "        y[i, :end_index] = sample[1:]   # Exclude the first token for y\n",
            "\n",
            "    # Ensure x and y are the correct shape (batch_size, block_size) if needed:\n",
            "    # Here, we truncate to `block_size` if samples are longer than `block_size`.\n",
            "    x = x[:, :model_config.block_size]\n",
            "    y = y[:, :model_config.block_size]\n",
            "\n",
            "    # x and y have to be\n",
            "    x = x.to(config.device)\n",
            "    y = y.to(config.device)\n",
            "\n",
            "    return x, y\n",
            "\n",
            "create_get_batch = lambda config, dataset, model_config: lambda split: get_batch_lambda(config, dataset, model_config, split)\n",
            "get_batch = create_get_batch(config, dataset, model_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "30294000"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "len(train_indices)*dataset[0][0].shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Resuming training from models/token_transformer\n",
                  "number of parameters: 0.88M\n",
                  "num decayed parameter tensors: 26, with 928,044 parameters\n",
                  "num non-decayed parameter tensors: 50, with 8,640 parameters\n",
                  "using fused AdamW: True\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "wandb version 0.17.2 is available!  To upgrade, please run:\n",
                     " $ pip install wandb --upgrade"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Tracking run with wandb version 0.16.6"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Run data is saved locally in <code>/home/luca/uni/master/adl4cv/wandb/run-20240617_213808-gdf0eqvu</code>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Syncing run <strong><a href='https://wandb.ai/adl-for-cv/naive_token_transformer/runs/gdf0eqvu' target=\"_blank\">run-2024-06-17-21-38-00</a></strong> to <a href='https://wandb.ai/adl-for-cv/naive_token_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     " View project at <a href='https://wandb.ai/adl-for-cv/naive_token_transformer' target=\"_blank\">https://wandb.ai/adl-for-cv/naive_token_transformer</a>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     " View run at <a href='https://wandb.ai/adl-for-cv/naive_token_transformer/runs/gdf0eqvu' target=\"_blank\">https://wandb.ai/adl-for-cv/naive_token_transformer/runs/gdf0eqvu</a>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 14500: train loss 2.4673, val loss 2.4504\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 14750: train loss 2.3959, val loss 2.3973\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15000: train loss 2.3804, val loss 2.4028\n",
                  "step 15250: train loss 2.3593, val loss 2.3853\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15500: train loss 2.3502, val loss 2.3727\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15750: train loss 2.3591, val loss 2.3627\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 16000: train loss 2.3513, val loss 2.3636\n",
                  "step 16250: train loss 2.3503, val loss 2.3767\n",
                  "step 16500: train loss 2.3580, val loss 2.3661\n"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m----> 4\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_nano_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvq_dicts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvq_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
                  "File \u001b[0;32m~/uni/master/adl4cv/training/training_nano_gpt.py:266\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(get_batch, config, model_config, vq, vq_config, early_stop)\u001b[0m\n\u001b[1;32m    262\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    263\u001b[0m         loss \u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m    264\u001b[0m     )  \u001b[38;5;66;03m# scale the loss to account for gradient accumulation\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[1;32m    268\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
                  "Cell \u001b[0;32mIn[9], line 65\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     61\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n\u001b[0;32m---> 65\u001b[0m create_get_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m config, dataset, model_config: \u001b[38;5;28;01mlambda\u001b[39;00m split: \u001b[43mget_batch_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m get_batch \u001b[38;5;241m=\u001b[39m create_get_batch(config, dataset, model_config)\n",
                  "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mget_batch_lambda\u001b[0;34m(config, dataset, model_config, split)\u001b[0m\n\u001b[1;32m     36\u001b[0m sample, label \u001b[38;5;241m=\u001b[39m dataset[idx]\n\u001b[1;32m     37\u001b[0m start_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([token_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOS\u001b[39m\u001b[38;5;124m\"\u001b[39m], token_dict[\u001b[38;5;28mstr\u001b[39m(label)]])\u001b[38;5;241m.\u001b[39mlong()  \u001b[38;5;66;03m# Start of sequence token\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#start_tokens = torch.Tensor([0]).long()  # Start of sequence token\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#sample = torch.cat((start_tokens, sample + 1), dim=0)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m samples\u001b[38;5;241m.\u001b[39mappend(sample)\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source": [
            "# Prepeare model parameters and train\n",
            "import wandb\n",
            "wandb.login()\n",
            "trained_model = training_nano_gpt.train(get_batch, config, model_config, vq, vq_dicts[\"vq_config\"])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "number of parameters: 0.88M\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([[False, False, False, False, False, False, False,  True,  True, False,\n",
                     "         False, False,  True, False, False, False, False, False, False, False,\n",
                     "          True, False, False,  True, False, False, False,  True, False, False,\n",
                     "         False, False, False,  True,  True,  True,  True,  True, False, False,\n",
                     "         False,  True,  True,  True, False,  True,  True,  True, False, False,\n",
                     "         False,  True, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False,  True,\n",
                     "         False, False,  True, False, False,  True, False, False, False, False,\n",
                     "         False, False,  True, False,  True, False,  True, False,  True, False,\n",
                     "         False, False, False, False, False, False, False,  True,  True, False,\n",
                     "         False,  True, False,  True, False, False, False, False, False, False,\n",
                     "         False, False,  True, False, False, False, False,  True,  True, False,\n",
                     "          True, False,  True,  True,  True, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "          True, False, False, False, False, False, False, False,  True,  True,\n",
                     "         False,  True, False, False,  True, False, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False, False, False,  True,\n",
                     "         False, False, False, False, False, False,  True, False, False, False,\n",
                     "         False,  True, False, False, False, False, False, False, False, False,\n",
                     "         False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
                     "         False,  True, False,  True, False,  True, False, False, False, False,\n",
                     "          True, False, False, False, False, False, False, False,  True,  True,\n",
                     "         False,  True, False, False, False, False, False,  True,  True, False,\n",
                     "         False, False, False, False, False,  True, False, False,  True, False,\n",
                     "          True, False,  True, False, False, False,  True,  True,  True,  True,\n",
                     "         False, False, False,  True,  True, False,  True, False,  True,  True,\n",
                     "         False,  True, False, False, False,  True, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False, False, False, False,\n",
                     "         False,  True,  True, False,  True,  True,  True,  True, False, False,\n",
                     "         False,  True, False, False,  True, False,  True,  True, False, False,\n",
                     "          True, False,  True,  True,  True,  True,  True,  True, False,  True,\n",
                     "          True, False,  True, False, False,  True, False, False, False, False,\n",
                     "          True, False, False,  True,  True, False, False, False,  True,  True,\n",
                     "         False, False, False, False,  True, False, False, False, False,  True,\n",
                     "         False, False, False, False,  True, False, False, False, False,  True,\n",
                     "         False, False, False, False, False,  True, False, False, False, False,\n",
                     "         False,  True, False, False, False, False, False, False, False, False,\n",
                     "          True,  True, False, False, False, False,  True, False, False, False,\n",
                     "          True, False, False, False,  True, False,  True,  True, False, False,\n",
                     "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True,  True,  True,  True,  True, False,  True, False,\n",
                     "         False, False, False, False, False,  True, False, False,  True,  True,\n",
                     "          True,  True, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False,  True, False, False, False, False,\n",
                     "         False,  True, False, False, False, False, False, False, False, False,\n",
                     "         False,  True, False, False, False, False,  True,  True, False, False,\n",
                     "         False, False,  True, False, False,  True, False,  True, False, False,\n",
                     "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
                     "         False,  True, False, False,  True, False, False, False, False,  True,\n",
                     "         False, False, False,  True,  True,  True, False, False, False, False,\n",
                     "         False,  True, False, False, False,  True, False, False, False, False,\n",
                     "          True,  True, False, False,  True, False, False,  True, False, False,\n",
                     "         False, False,  True, False, False,  True,  True,  True,  True, False,\n",
                     "         False,  True, False,  True,  True,  True, False, False,  True,  True,\n",
                     "          True]], device='cuda:0')"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from networks.nano_gpt import GPT\n",
            "from utils import get_default_device\n",
            "\n",
            "model_dict = torch.load(\"./models/token_transformer/ckpt.pt\")\n",
            "# Configuration\n",
            "idx = 3\n",
            "\n",
            "device = get_default_device()\n",
            "model = GPT(model_dict[\"model_args\"])\n",
            "model.to(device=device)\n",
            "model.load_state_dict(model_dict[\"model\"])\n",
            "model.eval()\n",
            "\n",
            "vq = VectorQuantize(**model_dict[\"vq_config\"])\n",
            "vq.load_state_dict(model_dict[\"vq_state_dict\"])\n",
            "vq.eval()\n",
            "\n",
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "\n",
            "\n",
            "sample = dataset[0][0]\n",
            "X, Y = get_batch('val')\n",
            "X, Y = (X[0].unsqueeze(0), Y[0].unsqueeze(0))\n",
            "pred, _ = model(X, Y)\n",
            "# Sanity Check\n",
            "# Should be all true except first/second element\n",
            "pred.argmax(dim=-1)==Y\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor(6, device='cuda:0')\n",
                  "tensor(0, device='cuda:0')\n",
                  "tensor(5, device='cuda:0')\n",
                  "tensor(5, device='cuda:0')\n"
               ]
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 2000x1000 with 4 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "from animation.util import backtransform_weights, reconstruct_image\n",
            "from networks.mlp_models import MLP3D\n",
            "\n",
            "ij_len = 2\n",
            "# Plotting the tensors as heatmaps in grayscale\n",
            "fig, axes = plt.subplots(ij_len, ij_len, figsize=(20, 10))\n",
            "\n",
            "SOS = torch.Tensor([[0]]).long().to(device)\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": None,\n",
            "}\n",
            "\n",
            "for i in range(ij_len):\n",
            "    for j in range(ij_len):\n",
            "\n",
            "        model.eval()\n",
            "        novel_tokens = model.generate(SOS, dataset[0][0].shape[0] + 1, temperature=1.0, top_k=None)[:, 1:]\n",
            "\n",
            "        print(novel_tokens[0][0])\n",
            "        novel_tokens = novel_tokens[:, 1:].unsqueeze(-1).to(\"cpu\")\n",
            "                                                                                                                         \n",
            "\n",
            "        max_similarity = 0\n",
            "        \"\"\"\n",
            "        for data in dataset:\n",
            "            similarity = (data[0].to(device)==novel_tokens.squeeze(-1).squeeze(0).to(device)).int().sum()\n",
            "            if similarity > max_similarity:\n",
            "                max_similarity = similarity\n",
            "        \"\"\"\n",
            "        #print(f\"Maximum Similarity of picture (i, j) {(i, j)}: {max_similarity}\")\n",
            "\n",
            "        novel_weights= vq.get_codes_from_indices((novel_tokens-11))\n",
            "\n",
            "        dataset_no_transform = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), **kwargs)\n",
            "        original_dict = dataset_no_transform[0][0]\n",
            "\n",
            "        reconstructed_dict = backtransform_weights(novel_weights, original_dict[\"state_dict\"])\n",
            "\n",
            "        mlp3d = MLP3D(**original_dict[\"model_config\"])\n",
            "        mlp3d.load_state_dict(reconstructed_dict)\n",
            "        reconstructed_tensor = reconstruct_image(mlp3d)\n",
            "\n",
            "        axes[i][j].imshow(reconstructed_tensor, cmap='gray', aspect='auto')\n",
            "\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.14"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
