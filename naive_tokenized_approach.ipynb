{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/luca/.cache/pypoetry/virtualenvs/adl4cv-OvNqwVNf-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n",
                  "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
                  "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluca-fanselau\u001b[0m (\u001b[33madl-for-cv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import torch\n",
            "from vector_quantize_pytorch import VectorQuantize\n",
            "import os\n",
            "from data.neural_field_datasets import MnistNeFDataset, TokenTransform\n",
            "from training import training_nano_gpt\n",
            "\n",
            "from networks.nano_gpt import GPTConfig\n",
            "\n",
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": 5,\n",
            "}\n",
            "\n",
            "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
            "data_root = os.path.join(dir_path, \"adl4cv\")\n",
            "\n",
            "# load used vector quantizer\n",
            "vq_dicts = torch.load(os.path.join(data_root, \"models\", \"vqs\", \"vq_mnist_vs_255.pt\"))\n",
            "vq = VectorQuantize(**vq_dicts[\"vq_config\"])\n",
            "vq.load_state_dict(vq_dicts[\"state_dict\"])\n",
            "\n",
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "config = training_nano_gpt.Config()\n",
            "config.learning_rate=4e-3\n",
            "config.max_iters = 60000\n",
            "config.weight_decay=0.1\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=config.max_iters\n",
            "config.warmup_iters=0.05*config.max_iters\n",
            "config.batch_size = 64\n",
            "config.gradient_accumulation_steps = 1\n",
            "config.init_from = \"scratch\"\n",
            "config.out_dir =\"models/token_transformer\"\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "config.eval_interval = 250\n",
            "\n",
            "model_config = GPTConfig(n_embd=108, block_size=len(dataset[0][0]), n_head=12, n_layer=6, vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1, dropout=0.15)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "#early_stopping = training_nano_gpt.EarlyStopper(20)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'model_config = GPTConfig(\\n    n_embd=120, \\n    block_size=len(dataset[0][0]), \\n    n_head=12, n_layer=6, \\n    vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1,\\n    dropout=0.0\\n    )'"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"\"\"model_config = GPTConfig(\n",
            "    n_embd=120, \n",
            "    block_size=len(dataset[0][0]), \n",
            "    n_head=12, n_layer=6, \n",
            "    vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1,\n",
            "    dropout=0.0\n",
            "    )\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Where to put?\n",
            "# Maybe adjust dataset to be able to work with splitting data and then rewrite TokenTransform \n",
            "# to do the job combined with pytorch dataloader (get_batch == __call__ of Dataloader)\n",
            "\n",
            "def create_split_indices(n, train_ratio=0.9):\n",
            "    # Generate a random permutation of indices from 0 to n-1\n",
            "    shuffled_indices = torch.randperm(n)\n",
            "    # Determine the cut-off for training data\n",
            "    train_size = int(train_ratio * n)\n",
            "    # Split indices into training and validation sets\n",
            "    train_indices = shuffled_indices[:train_size]\n",
            "    val_indices = shuffled_indices[train_size:]\n",
            "    return train_indices, val_indices\n",
            "\n",
            "train_indices, val_indices = create_split_indices(len(dataset))\n",
            "\n",
            "def get_batch_lambda(n, config, dataset, model_config, split):\n",
            "    batch_size = config.batch_size\n",
            "    \n",
            "    # Generate random indices for batch selection\n",
            "    indices = torch.randint(0, n, (batch_size,))\n",
            "\n",
            "    # Select indices based on the split\n",
            "    if split == 'train':\n",
            "        # Randomly select batch_size indices from the train_indices\n",
            "        indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
            "    elif split == 'val':\n",
            "        # Randomly select batch_size indices from the val_indices\n",
            "        indices = val_indices[torch.randint(0, len(val_indices), (batch_size,))]\n",
            "    \n",
            "    \n",
            "    # Initialize lists to hold the sequences and labels\n",
            "    samples = []\n",
            "    labels = []\n",
            "\n",
            "    # Collect samples and labels\n",
            "    for idx in indices:\n",
            "        sample, label = dataset[idx]\n",
            "        sos_token = torch.Tensor([0]).long()  # Start of sequence token\n",
            "        sample = torch.cat((sos_token, sample), dim=0)\n",
            "        samples.append(sample)\n",
            "        labels.append(label)\n",
            "\n",
            "    # Prepare the sequences for model input\n",
            "    max_len = samples[0].size(0)\n",
            "    x = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    y = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
            "    \n",
            "    for i, sample in enumerate(samples):\n",
            "        end_index = sample.size(0) - 1\n",
            "        x[i, :end_index] = sample[:-1]  # Exclude the last token for x\n",
            "        y[i, :end_index] = sample[1:]   # Exclude the first token for y\n",
            "\n",
            "    # Ensure x and y are the correct shape (batch_size, block_size) if needed:\n",
            "    # Here, we truncate to `block_size` if samples are longer than `block_size`.\n",
            "    x = x[:, :model_config.block_size]\n",
            "    y = y[:, :model_config.block_size]\n",
            "\n",
            "    # x and y have to be\n",
            "    x = x.to(config.device)\n",
            "    y = y.to(config.device)\n",
            "\n",
            "    return x, y\n",
            "\n",
            "create_get_batch = lambda n, config, dataset, model_config: lambda split: get_batch_lambda(n, config, dataset, model_config, split)\n",
            "get_batch = create_get_batch(1553, config, dataset, model_config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "2736558"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "len(train_indices)*dataset[0][0].shape[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Initializing a new model from scratch\n",
                  "number of parameters: 0.88M\n",
                  "num decayed parameter tensors: 26, with 928,044 parameters\n",
                  "num non-decayed parameter tensors: 50, with 8,640 parameters\n",
                  "using fused AdamW: True\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "wandb version 0.17.1 is available!  To upgrade, please run:\n",
                     " $ pip install wandb --upgrade"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Tracking run with wandb version 0.16.6"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Run data is saved locally in <code>/home/luca/uni/master/adl4cv/wandb/run-20240610_221617-z5gayyrw</code>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     "Syncing run <strong><a href='https://wandb.ai/adl-for-cv/naive_token_transformer/runs/z5gayyrw' target=\"_blank\">run-2024-06-10-22-16-16</a></strong> to <a href='https://wandb.ai/adl-for-cv/naive_token_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     " View project at <a href='https://wandb.ai/adl-for-cv/naive_token_transformer' target=\"_blank\">https://wandb.ai/adl-for-cv/naive_token_transformer</a>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/html": [
                     " View run at <a href='https://wandb.ai/adl-for-cv/naive_token_transformer/runs/z5gayyrw' target=\"_blank\">https://wandb.ai/adl-for-cv/naive_token_transformer/runs/z5gayyrw</a>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "step 0: train loss 5.5715, val loss 5.5707\n",
                  "step 250: train loss 3.9405, val loss 3.9542\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 500: train loss 3.5005, val loss 3.4845\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 750: train loss 3.4403, val loss 3.4476\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1000: train loss 3.3888, val loss 3.3996\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1250: train loss 3.3304, val loss 3.3339\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1500: train loss 3.2644, val loss 3.2779\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 1750: train loss 3.2013, val loss 3.2139\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2000: train loss 3.1212, val loss 3.1489\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2250: train loss 3.0842, val loss 3.0951\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2500: train loss 3.0250, val loss 3.0388\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 2750: train loss 2.9438, val loss 2.9798\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3000: train loss 2.8928, val loss 2.9211\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3250: train loss 2.8580, val loss 2.8593\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3500: train loss 2.8137, val loss 2.8295\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 3750: train loss 2.7785, val loss 2.8000\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 4000: train loss 2.7467, val loss 2.7821\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 4250: train loss 2.7099, val loss 2.7244\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 4500: train loss 2.7177, val loss 2.7609\n",
                  "step 4750: train loss 2.6960, val loss 2.7312\n",
                  "step 5000: train loss 2.6801, val loss 2.7120\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 5250: train loss 2.6863, val loss 2.6924\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 5500: train loss 2.6781, val loss 2.7046\n",
                  "step 5750: train loss 2.6507, val loss 2.6859\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 6000: train loss 2.6616, val loss 2.6942\n",
                  "step 6250: train loss 2.6293, val loss 2.6622\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 6500: train loss 2.6338, val loss 2.6853\n",
                  "step 6750: train loss 2.6281, val loss 2.6558\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 7000: train loss 2.6193, val loss 2.6620\n",
                  "step 7250: train loss 2.6122, val loss 2.6623\n",
                  "step 7500: train loss 2.6189, val loss 2.6399\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 7750: train loss 2.6035, val loss 2.6601\n",
                  "step 8000: train loss 2.6066, val loss 2.6473\n",
                  "step 8250: train loss 2.6016, val loss 2.6462\n",
                  "step 8500: train loss 2.5970, val loss 2.6384\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 8750: train loss 2.5916, val loss 2.6353\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 9000: train loss 2.6010, val loss 2.6301\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 9250: train loss 2.5940, val loss 2.6384\n",
                  "step 9500: train loss 2.5922, val loss 2.6317\n",
                  "step 9750: train loss 2.5905, val loss 2.6342\n",
                  "step 10000: train loss 2.5791, val loss 2.6270\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 10250: train loss 2.5834, val loss 2.6151\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 10500: train loss 2.5783, val loss 2.6225\n",
                  "step 10750: train loss 2.5710, val loss 2.6302\n",
                  "step 11000: train loss 2.5661, val loss 2.6113\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 11250: train loss 2.5633, val loss 2.6054\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 11500: train loss 2.5661, val loss 2.6098\n",
                  "step 11750: train loss 2.5608, val loss 2.6005\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 12000: train loss 2.5696, val loss 2.6059\n",
                  "step 12250: train loss 2.5565, val loss 2.6100\n",
                  "step 12500: train loss 2.5628, val loss 2.6172\n",
                  "step 12750: train loss 2.5608, val loss 2.6038\n",
                  "step 13000: train loss 2.5477, val loss 2.5954\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 13250: train loss 2.5618, val loss 2.6045\n",
                  "step 13500: train loss 2.5530, val loss 2.6036\n",
                  "step 13750: train loss 2.5508, val loss 2.6063\n",
                  "step 14000: train loss 2.5521, val loss 2.5996\n",
                  "step 14250: train loss 2.5465, val loss 2.5967\n",
                  "step 14500: train loss 2.5529, val loss 2.5975\n",
                  "step 14750: train loss 2.5534, val loss 2.5965\n",
                  "step 15000: train loss 2.5476, val loss 2.5931\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15250: train loss 2.5482, val loss 2.5843\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 15500: train loss 2.5371, val loss 2.6048\n",
                  "step 15750: train loss 2.5464, val loss 2.6082\n",
                  "step 16000: train loss 2.5613, val loss 2.5963\n",
                  "step 16250: train loss 2.5469, val loss 2.5939\n",
                  "step 16500: train loss 2.5348, val loss 2.5960\n",
                  "step 16750: train loss 2.5469, val loss 2.5975\n",
                  "step 17000: train loss 2.5393, val loss 2.5919\n",
                  "step 17250: train loss 2.5325, val loss 2.5980\n",
                  "step 17500: train loss 2.5481, val loss 2.5972\n",
                  "step 17750: train loss 2.5304, val loss 2.5870\n",
                  "step 18000: train loss 2.5359, val loss 2.5880\n",
                  "step 18250: train loss 2.5256, val loss 2.5833\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 18500: train loss 2.5333, val loss 2.5903\n",
                  "step 18750: train loss 2.5286, val loss 2.5916\n",
                  "step 19000: train loss 2.5432, val loss 2.5845\n",
                  "step 19250: train loss 2.5288, val loss 2.5872\n",
                  "step 19500: train loss 2.5257, val loss 2.5816\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 19750: train loss 2.5338, val loss 2.5872\n",
                  "step 20000: train loss 2.5138, val loss 2.5769\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 20250: train loss 2.5265, val loss 2.5724\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 20500: train loss 2.5213, val loss 2.5831\n",
                  "step 20750: train loss 2.5282, val loss 2.5667\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 21000: train loss 2.5251, val loss 2.5892\n",
                  "step 21250: train loss 2.5247, val loss 2.5773\n",
                  "step 21500: train loss 2.5106, val loss 2.5754\n",
                  "step 21750: train loss 2.5356, val loss 2.5804\n",
                  "step 22000: train loss 2.5153, val loss 2.5723\n",
                  "step 22250: train loss 2.5260, val loss 2.5680\n",
                  "step 22500: train loss 2.5016, val loss 2.5684\n",
                  "step 22750: train loss 2.5238, val loss 2.5674\n",
                  "step 23000: train loss 2.5281, val loss 2.5683\n",
                  "step 23250: train loss 2.5144, val loss 2.5759\n",
                  "step 23500: train loss 2.5096, val loss 2.5632\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 23750: train loss 2.5162, val loss 2.5657\n",
                  "step 24000: train loss 2.5058, val loss 2.5694\n",
                  "step 24250: train loss 2.5101, val loss 2.5714\n",
                  "step 24500: train loss 2.5177, val loss 2.5642\n",
                  "step 24750: train loss 2.5102, val loss 2.5688\n",
                  "step 25000: train loss 2.5064, val loss 2.5725\n",
                  "step 25250: train loss 2.5003, val loss 2.5660\n",
                  "step 25500: train loss 2.4888, val loss 2.5530\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 25750: train loss 2.5085, val loss 2.5684\n",
                  "step 26000: train loss 2.4993, val loss 2.5471\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 26250: train loss 2.4878, val loss 2.5491\n",
                  "step 26500: train loss 2.5021, val loss 2.5593\n",
                  "step 26750: train loss 2.5039, val loss 2.5775\n",
                  "step 27000: train loss 2.5047, val loss 2.5575\n",
                  "step 27250: train loss 2.5047, val loss 2.5573\n",
                  "step 27500: train loss 2.5085, val loss 2.5533\n",
                  "step 27750: train loss 2.5039, val loss 2.5606\n",
                  "step 28000: train loss 2.5097, val loss 2.5459\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 28250: train loss 2.4930, val loss 2.5637\n",
                  "step 28500: train loss 2.5071, val loss 2.5648\n",
                  "step 28750: train loss 2.4951, val loss 2.5544\n",
                  "step 29000: train loss 2.4964, val loss 2.5508\n",
                  "step 29250: train loss 2.4869, val loss 2.5527\n",
                  "step 29500: train loss 2.5018, val loss 2.5487\n",
                  "step 29750: train loss 2.4923, val loss 2.5436\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 30000: train loss 2.4879, val loss 2.5556\n",
                  "step 30250: train loss 2.4882, val loss 2.5536\n",
                  "step 30500: train loss 2.4815, val loss 2.5628\n",
                  "step 30750: train loss 2.4986, val loss 2.5500\n",
                  "step 31000: train loss 2.4854, val loss 2.5338\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 31250: train loss 2.4903, val loss 2.5494\n",
                  "step 31500: train loss 2.4998, val loss 2.5670\n",
                  "step 31750: train loss 2.4828, val loss 2.5413\n",
                  "step 32000: train loss 2.4936, val loss 2.5452\n",
                  "step 32250: train loss 2.4755, val loss 2.5411\n",
                  "step 32500: train loss 2.4822, val loss 2.5376\n",
                  "step 32750: train loss 2.4834, val loss 2.5566\n",
                  "step 33000: train loss 2.4788, val loss 2.5545\n",
                  "step 33250: train loss 2.4747, val loss 2.5466\n",
                  "step 33500: train loss 2.4773, val loss 2.5379\n",
                  "step 33750: train loss 2.4835, val loss 2.5357\n",
                  "step 34000: train loss 2.4771, val loss 2.5346\n",
                  "step 34250: train loss 2.4607, val loss 2.5363\n",
                  "step 34500: train loss 2.4684, val loss 2.5457\n",
                  "step 34750: train loss 2.4752, val loss 2.5506\n",
                  "step 35000: train loss 2.4769, val loss 2.5260\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 35250: train loss 2.4761, val loss 2.5440\n",
                  "step 35500: train loss 2.4669, val loss 2.5283\n",
                  "step 35750: train loss 2.4618, val loss 2.5248\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 36000: train loss 2.4754, val loss 2.5317\n",
                  "step 36250: train loss 2.4535, val loss 2.5358\n",
                  "step 36500: train loss 2.4591, val loss 2.5405\n",
                  "step 36750: train loss 2.4676, val loss 2.5308\n",
                  "step 37000: train loss 2.4727, val loss 2.5197\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 37250: train loss 2.4666, val loss 2.5267\n",
                  "step 37500: train loss 2.4529, val loss 2.5293\n",
                  "step 37750: train loss 2.4636, val loss 2.5307\n",
                  "step 38000: train loss 2.4678, val loss 2.5175\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 38250: train loss 2.4682, val loss 2.5328\n",
                  "step 38500: train loss 2.4490, val loss 2.5291\n",
                  "step 38750: train loss 2.4604, val loss 2.5283\n",
                  "step 39000: train loss 2.4601, val loss 2.5117\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 39250: train loss 2.4597, val loss 2.5323\n",
                  "step 39500: train loss 2.4570, val loss 2.5212\n",
                  "step 39750: train loss 2.4570, val loss 2.5255\n",
                  "step 40000: train loss 2.4519, val loss 2.5367\n",
                  "step 40250: train loss 2.4546, val loss 2.5405\n",
                  "step 40500: train loss 2.4498, val loss 2.5274\n",
                  "step 40750: train loss 2.4508, val loss 2.5193\n",
                  "step 41000: train loss 2.4348, val loss 2.5227\n",
                  "step 41250: train loss 2.4415, val loss 2.5196\n",
                  "step 41500: train loss 2.4553, val loss 2.5213\n",
                  "step 41750: train loss 2.4559, val loss 2.5234\n",
                  "step 42000: train loss 2.4422, val loss 2.5183\n",
                  "step 42250: train loss 2.4454, val loss 2.5215\n",
                  "step 42500: train loss 2.4290, val loss 2.5248\n",
                  "step 42750: train loss 2.4536, val loss 2.5140\n",
                  "step 43000: train loss 2.4387, val loss 2.5150\n",
                  "step 43250: train loss 2.4329, val loss 2.5093\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 43500: train loss 2.4385, val loss 2.5023\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 43750: train loss 2.4333, val loss 2.5253\n",
                  "step 44000: train loss 2.4304, val loss 2.5023\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 44250: train loss 2.4469, val loss 2.5077\n",
                  "step 44500: train loss 2.4433, val loss 2.5254\n",
                  "step 44750: train loss 2.4342, val loss 2.5089\n",
                  "step 45000: train loss 2.4398, val loss 2.5069\n",
                  "step 45250: train loss 2.4139, val loss 2.5098\n",
                  "step 45500: train loss 2.4294, val loss 2.5094\n",
                  "step 45750: train loss 2.4339, val loss 2.4906\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 46000: train loss 2.4297, val loss 2.5108\n",
                  "step 46250: train loss 2.4308, val loss 2.5038\n",
                  "step 46500: train loss 2.4388, val loss 2.5047\n",
                  "step 46750: train loss 2.4272, val loss 2.5179\n",
                  "step 47000: train loss 2.4296, val loss 2.5018\n",
                  "step 47250: train loss 2.4347, val loss 2.5009\n",
                  "step 47500: train loss 2.4351, val loss 2.5027\n",
                  "step 47750: train loss 2.4286, val loss 2.4936\n",
                  "step 48000: train loss 2.4142, val loss 2.5133\n",
                  "step 48250: train loss 2.4276, val loss 2.5028\n",
                  "step 48500: train loss 2.4300, val loss 2.5125\n",
                  "step 48750: train loss 2.4273, val loss 2.5060\n",
                  "step 49000: train loss 2.4260, val loss 2.5080\n",
                  "step 49250: train loss 2.4201, val loss 2.5096\n",
                  "step 49500: train loss 2.4194, val loss 2.4952\n",
                  "step 49750: train loss 2.4099, val loss 2.4943\n",
                  "step 50000: train loss 2.4098, val loss 2.4974\n",
                  "step 50250: train loss 2.4248, val loss 2.5009\n",
                  "step 50500: train loss 2.4226, val loss 2.4957\n",
                  "step 50750: train loss 2.4223, val loss 2.4992\n",
                  "step 51000: train loss 2.4240, val loss 2.4796\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 51250: train loss 2.4144, val loss 2.4935\n",
                  "step 51500: train loss 2.4088, val loss 2.5078\n",
                  "step 51750: train loss 2.4155, val loss 2.4985\n",
                  "step 52000: train loss 2.4091, val loss 2.4947\n",
                  "step 52250: train loss 2.4250, val loss 2.4951\n",
                  "step 52500: train loss 2.4107, val loss 2.4982\n",
                  "step 52750: train loss 2.4120, val loss 2.4930\n",
                  "step 53000: train loss 2.4042, val loss 2.4937\n",
                  "step 53250: train loss 2.4127, val loss 2.4829\n",
                  "step 53500: train loss 2.4181, val loss 2.4968\n",
                  "step 53750: train loss 2.4076, val loss 2.5098\n",
                  "step 54000: train loss 2.4195, val loss 2.4929\n",
                  "step 54250: train loss 2.4013, val loss 2.4897\n",
                  "step 54500: train loss 2.4107, val loss 2.4879\n",
                  "step 54750: train loss 2.4122, val loss 2.4873\n",
                  "step 55000: train loss 2.4092, val loss 2.4765\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 55250: train loss 2.4010, val loss 2.4930\n",
                  "step 55500: train loss 2.4069, val loss 2.4942\n",
                  "step 55750: train loss 2.4164, val loss 2.4846\n",
                  "step 56000: train loss 2.4090, val loss 2.4948\n",
                  "step 56250: train loss 2.3928, val loss 2.4875\n",
                  "step 56500: train loss 2.4022, val loss 2.4827\n",
                  "step 56750: train loss 2.4114, val loss 2.4865\n",
                  "step 57000: train loss 2.3974, val loss 2.5016\n",
                  "step 57250: train loss 2.4110, val loss 2.4861\n",
                  "step 57500: train loss 2.4031, val loss 2.4810\n",
                  "step 57750: train loss 2.4007, val loss 2.4885\n",
                  "step 58000: train loss 2.4016, val loss 2.4750\n",
                  "saving checkpoint to models/token_transformer\n",
                  "step 58250: train loss 2.4113, val loss 2.4899\n",
                  "step 58500: train loss 2.4094, val loss 2.4864\n",
                  "step 58750: train loss 2.4129, val loss 2.5020\n",
                  "step 59000: train loss 2.3998, val loss 2.4948\n",
                  "step 59250: train loss 2.4131, val loss 2.4882\n",
                  "step 59500: train loss 2.4184, val loss 2.4919\n",
                  "step 59750: train loss 2.4043, val loss 2.4949\n",
                  "step 60000: train loss 2.4100, val loss 2.4744\n",
                  "saving checkpoint to models/token_transformer\n"
               ]
            }
         ],
         "source": [
            "# Prepeare model parameters and train\n",
            "trained_model = training_nano_gpt.train(get_batch, config, model_config, vq, vq_dicts[\"vq_config\"])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "ename": "NameError",
               "evalue": "name 'model_dict' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_dict\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
                  "\u001b[0;31mNameError\u001b[0m: name 'model_dict' is not defined"
               ]
            }
         ],
         "source": [
            "model_dict.keys()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "number of parameters: 0.88M\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "tensor([[False, False, False, False, False,  True, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False,  True,  True, False,  True, False,  True,\n",
                     "         False,  True,  True, False, False,  True, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False,  True,  True,\n",
                     "         False,  True,  True, False, False, False, False,  True,  True,  True,\n",
                     "         False, False,  True, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False,  True, False, False, False, False, False,\n",
                     "          True,  True, False, False, False,  True, False, False,  True, False,\n",
                     "         False, False, False, False, False,  True, False, False, False, False,\n",
                     "         False, False, False,  True, False, False, False,  True, False, False,\n",
                     "          True, False, False, False, False, False, False,  True, False, False,\n",
                     "         False, False,  True, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False,  True, False, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False,  True, False,  True, False, False, False, False, False, False,\n",
                     "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
                     "         False, False,  True, False, False, False, False,  True,  True,  True,\n",
                     "         False, False, False, False,  True, False,  True, False,  True, False,\n",
                     "         False, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False,  True, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False,  True, False, False, False,\n",
                     "         False, False,  True, False, False, False, False, False,  True, False,\n",
                     "          True,  True, False, False, False, False, False, False, False, False,\n",
                     "         False,  True, False, False,  True, False, False, False, False,  True,\n",
                     "          True,  True, False,  True, False,  True,  True, False, False, False,\n",
                     "          True, False,  True, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False,  True, False, False,  True,\n",
                     "         False, False, False, False, False, False,  True, False, False, False,\n",
                     "         False, False,  True, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False, False, False, False, False,  True, False,\n",
                     "          True, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False, False,  True, False, False, False, False,  True,\n",
                     "          True, False, False, False, False, False, False, False, False, False,\n",
                     "          True, False, False, False, False, False, False, False, False, False,\n",
                     "         False, False, False,  True, False, False,  True, False, False, False,\n",
                     "         False,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True, False,  True,  True, False, False, False, False,\n",
                     "         False, False, False,  True,  True, False, False, False,  True,  True,\n",
                     "          True, False, False,  True, False, False, False, False, False, False,\n",
                     "         False, False, False, False,  True, False, False, False,  True,  True,\n",
                     "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
                     "          True,  True,  True,  True, False, False, False, False, False, False,\n",
                     "         False, False, False, False,  True, False,  True, False, False, False,\n",
                     "         False, False, False, False,  True, False,  True, False, False,  True,\n",
                     "         False, False,  True,  True,  True, False, False, False, False, False,\n",
                     "         False, False, False,  True, False, False, False, False,  True, False,\n",
                     "         False, False, False, False, False, False, False, False, False,  True,\n",
                     "         False, False, False, False,  True, False, False, False, False,  True,\n",
                     "         False, False,  True, False, False, False,  True, False, False,  True,\n",
                     "         False, False, False,  True, False,  True, False, False, False, False,\n",
                     "          True, False,  True,  True, False,  True, False, False,  True, False,\n",
                     "         False]], device='cuda:0')"
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import torch\n",
            "from networks.nano_gpt import GPT\n",
            "from utils import get_default_device\n",
            "\n",
            "model_dict = torch.load(\"./models/token_transformer/ckpt.pt\")\n",
            "# Configuration\n",
            "idx = 3\n",
            "\n",
            "device = get_default_device()\n",
            "model = GPT(model_dict[\"model_args\"])\n",
            "model.to(device=device)\n",
            "model.load_state_dict(model_dict[\"model\"])\n",
            "model.eval()\n",
            "\n",
            "vq = VectorQuantize(**model_dict[\"vq_config\"])\n",
            "vq.load_state_dict(model_dict[\"vq_state_dict\"])\n",
            "vq.eval()\n",
            "\n",
            "dataset = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), transform=TokenTransform(vq), **kwargs)\n",
            "\n",
            "\n",
            "sample = dataset[0][0]\n",
            "X, Y = get_batch(\"\")\n",
            "X, Y = (X[0].unsqueeze(0), Y[0].unsqueeze(0))\n",
            "pred, _ = model(X, Y)\n",
            "# Sanity Check\n",
            "# Should be all true except first/second element\n",
            "pred.argmax(dim=-1)==Y\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "tensor([[[ 63],\n",
                     "         [ 32],\n",
                     "         [146],\n",
                     "         [  6],\n",
                     "         [ 90],\n",
                     "         [162],\n",
                     "         [ 58],\n",
                     "         [130],\n",
                     "         [ 24],\n",
                     "         [ 21],\n",
                     "         [ 58],\n",
                     "         [218],\n",
                     "         [ 16],\n",
                     "         [ 89],\n",
                     "         [243],\n",
                     "         [ 85],\n",
                     "         [ 59],\n",
                     "         [148],\n",
                     "         [200],\n",
                     "         [ 32],\n",
                     "         [150],\n",
                     "         [252],\n",
                     "         [ 94],\n",
                     "         [ 94],\n",
                     "         [220],\n",
                     "         [107],\n",
                     "         [107],\n",
                     "         [ 26],\n",
                     "         [ 18],\n",
                     "         [ 89],\n",
                     "         [153],\n",
                     "         [103],\n",
                     "         [ 89],\n",
                     "         [  7],\n",
                     "         [233],\n",
                     "         [181],\n",
                     "         [248],\n",
                     "         [106],\n",
                     "         [  9],\n",
                     "         [161],\n",
                     "         [210],\n",
                     "         [127],\n",
                     "         [152],\n",
                     "         [181],\n",
                     "         [ 33],\n",
                     "         [160],\n",
                     "         [ 82],\n",
                     "         [201],\n",
                     "         [ 24],\n",
                     "         [178],\n",
                     "         [112],\n",
                     "         [253],\n",
                     "         [245],\n",
                     "         [ 55],\n",
                     "         [ 57],\n",
                     "         [102],\n",
                     "         [ 92],\n",
                     "         [ 69],\n",
                     "         [109],\n",
                     "         [ 23],\n",
                     "         [255],\n",
                     "         [ 57],\n",
                     "         [105],\n",
                     "         [ 95],\n",
                     "         [  7],\n",
                     "         [109],\n",
                     "         [109],\n",
                     "         [131],\n",
                     "         [ 80],\n",
                     "         [225],\n",
                     "         [161],\n",
                     "         [ 49],\n",
                     "         [ 16],\n",
                     "         [109],\n",
                     "         [170],\n",
                     "         [102],\n",
                     "         [178],\n",
                     "         [139],\n",
                     "         [ 92],\n",
                     "         [127],\n",
                     "         [212],\n",
                     "         [ 23],\n",
                     "         [138],\n",
                     "         [228],\n",
                     "         [103],\n",
                     "         [ 32],\n",
                     "         [  7],\n",
                     "         [109],\n",
                     "         [  9],\n",
                     "         [ 32],\n",
                     "         [ 82],\n",
                     "         [ 94],\n",
                     "         [102],\n",
                     "         [216],\n",
                     "         [152],\n",
                     "         [215],\n",
                     "         [ 24],\n",
                     "         [210],\n",
                     "         [130],\n",
                     "         [ 62],\n",
                     "         [213],\n",
                     "         [ 57],\n",
                     "         [191],\n",
                     "         [ 46],\n",
                     "         [ 91],\n",
                     "         [201],\n",
                     "         [138],\n",
                     "         [212],\n",
                     "         [ 39],\n",
                     "         [ 89],\n",
                     "         [194],\n",
                     "         [180],\n",
                     "         [ 57],\n",
                     "         [241],\n",
                     "         [182],\n",
                     "         [ 59],\n",
                     "         [177],\n",
                     "         [ 73],\n",
                     "         [161],\n",
                     "         [110],\n",
                     "         [ 38],\n",
                     "         [ 94],\n",
                     "         [ 94],\n",
                     "         [139],\n",
                     "         [ 32],\n",
                     "         [ 39],\n",
                     "         [ 39],\n",
                     "         [209],\n",
                     "         [112],\n",
                     "         [188],\n",
                     "         [138],\n",
                     "         [114],\n",
                     "         [ 69],\n",
                     "         [ 26],\n",
                     "         [212],\n",
                     "         [148],\n",
                     "         [ 35],\n",
                     "         [148],\n",
                     "         [ 18],\n",
                     "         [ 63],\n",
                     "         [ 18],\n",
                     "         [ 89],\n",
                     "         [ 39],\n",
                     "         [239],\n",
                     "         [126],\n",
                     "         [244],\n",
                     "         [ 24],\n",
                     "         [177],\n",
                     "         [242],\n",
                     "         [ 59],\n",
                     "         [ 56],\n",
                     "         [ 58],\n",
                     "         [200],\n",
                     "         [226],\n",
                     "         [148],\n",
                     "         [ 63],\n",
                     "         [178],\n",
                     "         [ 89],\n",
                     "         [127],\n",
                     "         [203],\n",
                     "         [ 32],\n",
                     "         [ 66],\n",
                     "         [218],\n",
                     "         [ 36],\n",
                     "         [150],\n",
                     "         [249],\n",
                     "         [161],\n",
                     "         [153],\n",
                     "         [233],\n",
                     "         [ 16],\n",
                     "         [ 35],\n",
                     "         [  7],\n",
                     "         [ 39],\n",
                     "         [139],\n",
                     "         [203],\n",
                     "         [139],\n",
                     "         [ 94],\n",
                     "         [192],\n",
                     "         [109],\n",
                     "         [102],\n",
                     "         [233],\n",
                     "         [ 11],\n",
                     "         [109],\n",
                     "         [ 72],\n",
                     "         [201],\n",
                     "         [ 39],\n",
                     "         [ 23],\n",
                     "         [157],\n",
                     "         [105],\n",
                     "         [170],\n",
                     "         [180],\n",
                     "         [203],\n",
                     "         [ 77],\n",
                     "         [121],\n",
                     "         [104],\n",
                     "         [ 47],\n",
                     "         [ 26],\n",
                     "         [ 56],\n",
                     "         [231],\n",
                     "         [ 56],\n",
                     "         [234],\n",
                     "         [  2],\n",
                     "         [ 11],\n",
                     "         [172],\n",
                     "         [205],\n",
                     "         [248],\n",
                     "         [217],\n",
                     "         [159],\n",
                     "         [ 24],\n",
                     "         [244],\n",
                     "         [244],\n",
                     "         [ 57],\n",
                     "         [149],\n",
                     "         [121],\n",
                     "         [113],\n",
                     "         [ 31],\n",
                     "         [ 73],\n",
                     "         [ 36],\n",
                     "         [178],\n",
                     "         [ 72],\n",
                     "         [ 89],\n",
                     "         [152],\n",
                     "         [226],\n",
                     "         [212],\n",
                     "         [144],\n",
                     "         [218],\n",
                     "         [151],\n",
                     "         [149],\n",
                     "         [136],\n",
                     "         [ 99],\n",
                     "         [ 41],\n",
                     "         [ 49],\n",
                     "         [138],\n",
                     "         [ 35],\n",
                     "         [252],\n",
                     "         [ 78],\n",
                     "         [203],\n",
                     "         [ 95],\n",
                     "         [ 84],\n",
                     "         [152],\n",
                     "         [ 63],\n",
                     "         [ 89],\n",
                     "         [200],\n",
                     "         [ 95],\n",
                     "         [252],\n",
                     "         [166],\n",
                     "         [180],\n",
                     "         [131],\n",
                     "         [190],\n",
                     "         [233],\n",
                     "         [151],\n",
                     "         [ 16],\n",
                     "         [ 39],\n",
                     "         [102],\n",
                     "         [127],\n",
                     "         [198],\n",
                     "         [ 39],\n",
                     "         [168],\n",
                     "         [132],\n",
                     "         [ 71],\n",
                     "         [208],\n",
                     "         [253],\n",
                     "         [190],\n",
                     "         [164],\n",
                     "         [162],\n",
                     "         [153],\n",
                     "         [103],\n",
                     "         [150],\n",
                     "         [ 35],\n",
                     "         [195],\n",
                     "         [228],\n",
                     "         [200],\n",
                     "         [ 94],\n",
                     "         [246],\n",
                     "         [ 89],\n",
                     "         [ 14],\n",
                     "         [112],\n",
                     "         [191],\n",
                     "         [152],\n",
                     "         [200],\n",
                     "         [102],\n",
                     "         [ 63],\n",
                     "         [148],\n",
                     "         [157],\n",
                     "         [141],\n",
                     "         [ 63],\n",
                     "         [ 56],\n",
                     "         [255],\n",
                     "         [235],\n",
                     "         [162],\n",
                     "         [102],\n",
                     "         [ 57],\n",
                     "         [132],\n",
                     "         [178],\n",
                     "         [ 69],\n",
                     "         [250],\n",
                     "         [190],\n",
                     "         [ 88],\n",
                     "         [203],\n",
                     "         [ 36],\n",
                     "         [179],\n",
                     "         [ 43],\n",
                     "         [ 89],\n",
                     "         [141],\n",
                     "         [ 23],\n",
                     "         [ 98],\n",
                     "         [ 89],\n",
                     "         [ 67],\n",
                     "         [ 77],\n",
                     "         [ 39],\n",
                     "         [ 39],\n",
                     "         [ 89],\n",
                     "         [148],\n",
                     "         [207],\n",
                     "         [ 39],\n",
                     "         [ 67],\n",
                     "         [ 76],\n",
                     "         [152],\n",
                     "         [ 89],\n",
                     "         [232],\n",
                     "         [152],\n",
                     "         [204],\n",
                     "         [ 35],\n",
                     "         [ 89],\n",
                     "         [ 18],\n",
                     "         [  8],\n",
                     "         [ 32],\n",
                     "         [112],\n",
                     "         [210],\n",
                     "         [ 63],\n",
                     "         [212],\n",
                     "         [148],\n",
                     "         [ 81],\n",
                     "         [148],\n",
                     "         [229],\n",
                     "         [ 39],\n",
                     "         [148],\n",
                     "         [174],\n",
                     "         [148],\n",
                     "         [178],\n",
                     "         [245],\n",
                     "         [ 89],\n",
                     "         [ 69],\n",
                     "         [ 94],\n",
                     "         [ 32],\n",
                     "         [148],\n",
                     "         [170],\n",
                     "         [ 27],\n",
                     "         [ 60],\n",
                     "         [ 82],\n",
                     "         [116],\n",
                     "         [241],\n",
                     "         [ 58],\n",
                     "         [ 30],\n",
                     "         [ 32],\n",
                     "         [ 35],\n",
                     "         [248],\n",
                     "         [230],\n",
                     "         [ 82],\n",
                     "         [ 58],\n",
                     "         [ 64],\n",
                     "         [ 40],\n",
                     "         [ 39],\n",
                     "         [209],\n",
                     "         [245],\n",
                     "         [ 32],\n",
                     "         [131],\n",
                     "         [ 39],\n",
                     "         [255],\n",
                     "         [ 67],\n",
                     "         [161],\n",
                     "         [127],\n",
                     "         [110],\n",
                     "         [ 40],\n",
                     "         [243],\n",
                     "         [233],\n",
                     "         [252],\n",
                     "         [ 89],\n",
                     "         [239],\n",
                     "         [127],\n",
                     "         [253],\n",
                     "         [245],\n",
                     "         [144],\n",
                     "         [ 89],\n",
                     "         [127],\n",
                     "         [ 67],\n",
                     "         [ 64],\n",
                     "         [ 43],\n",
                     "         [221],\n",
                     "         [ 61],\n",
                     "         [213],\n",
                     "         [ 57],\n",
                     "         [226],\n",
                     "         [167],\n",
                     "         [214],\n",
                     "         [245],\n",
                     "         [162],\n",
                     "         [252],\n",
                     "         [192],\n",
                     "         [ 81],\n",
                     "         [102],\n",
                     "         [ 93],\n",
                     "         [ 90],\n",
                     "         [203],\n",
                     "         [253],\n",
                     "         [ 81],\n",
                     "         [176],\n",
                     "         [  9],\n",
                     "         [225],\n",
                     "         [222],\n",
                     "         [  3],\n",
                     "         [103],\n",
                     "         [178],\n",
                     "         [249],\n",
                     "         [210],\n",
                     "         [153],\n",
                     "         [ 41],\n",
                     "         [232],\n",
                     "         [170],\n",
                     "         [ 56],\n",
                     "         [106],\n",
                     "         [ 82],\n",
                     "         [  8],\n",
                     "         [221],\n",
                     "         [241],\n",
                     "         [159],\n",
                     "         [236],\n",
                     "         [110],\n",
                     "         [188],\n",
                     "         [ 16],\n",
                     "         [ 73],\n",
                     "         [248],\n",
                     "         [180],\n",
                     "         [ 27],\n",
                     "         [253],\n",
                     "         [  7],\n",
                     "         [ 39],\n",
                     "         [ 57],\n",
                     "         [102],\n",
                     "         [ 83],\n",
                     "         [ 78],\n",
                     "         [  6],\n",
                     "         [127],\n",
                     "         [198],\n",
                     "         [ 50],\n",
                     "         [152],\n",
                     "         [ 35],\n",
                     "         [148],\n",
                     "         [ 39],\n",
                     "         [ 57],\n",
                     "         [ 63],\n",
                     "         [ 51],\n",
                     "         [ 19],\n",
                     "         [231],\n",
                     "         [118],\n",
                     "         [221],\n",
                     "         [203],\n",
                     "         [208],\n",
                     "         [ 22],\n",
                     "         [ 60],\n",
                     "         [207],\n",
                     "         [ 82],\n",
                     "         [ 11],\n",
                     "         [ 38],\n",
                     "         [209],\n",
                     "         [ 27],\n",
                     "         [232],\n",
                     "         [  8],\n",
                     "         [  7],\n",
                     "         [ 23],\n",
                     "         [ 57],\n",
                     "         [203],\n",
                     "         [ 63],\n",
                     "         [180],\n",
                     "         [157],\n",
                     "         [ 85],\n",
                     "         [206],\n",
                     "         [203],\n",
                     "         [ 45],\n",
                     "         [ 85],\n",
                     "         [177],\n",
                     "         [ 88],\n",
                     "         [122],\n",
                     "         [ 12],\n",
                     "         [ 82],\n",
                     "         [ 45],\n",
                     "         [225],\n",
                     "         [ 21],\n",
                     "         [ 12],\n",
                     "         [  2],\n",
                     "         [103],\n",
                     "         [203],\n",
                     "         [  8],\n",
                     "         [126],\n",
                     "         [ 82],\n",
                     "         [ 17],\n",
                     "         [ 37],\n",
                     "         [ 67],\n",
                     "         [ 23],\n",
                     "         [245],\n",
                     "         [105],\n",
                     "         [215],\n",
                     "         [199],\n",
                     "         [220],\n",
                     "         [ 30],\n",
                     "         [181],\n",
                     "         [ 66],\n",
                     "         [ 89],\n",
                     "         [169],\n",
                     "         [ 66],\n",
                     "         [ 89],\n",
                     "         [ 34],\n",
                     "         [160],\n",
                     "         [204],\n",
                     "         [214],\n",
                     "         [ 94],\n",
                     "         [137],\n",
                     "         [ 23],\n",
                     "         [126],\n",
                     "         [ 37],\n",
                     "         [ 35],\n",
                     "         [153],\n",
                     "         [255],\n",
                     "         [139],\n",
                     "         [  9],\n",
                     "         [ 54],\n",
                     "         [138],\n",
                     "         [ 89],\n",
                     "         [ 78],\n",
                     "         [113],\n",
                     "         [ 51],\n",
                     "         [ 38],\n",
                     "         [172],\n",
                     "         [ 19],\n",
                     "         [141],\n",
                     "         [252],\n",
                     "         [132],\n",
                     "         [224],\n",
                     "         [  3],\n",
                     "         [122],\n",
                     "         [ 46],\n",
                     "         [181],\n",
                     "         [112],\n",
                     "         [152],\n",
                     "         [243],\n",
                     "         [177],\n",
                     "         [ 37],\n",
                     "         [187],\n",
                     "         [ 89],\n",
                     "         [109],\n",
                     "         [251],\n",
                     "         [245],\n",
                     "         [170],\n",
                     "         [ 23],\n",
                     "         [132],\n",
                     "         [145],\n",
                     "         [110],\n",
                     "         [112],\n",
                     "         [232],\n",
                     "         [111],\n",
                     "         [169]]], device='cuda:0')"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "pred.argmax(dim=-1).unsqueeze(-1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Maximum Similarity of picture (i, j) (0, 0): 133\n",
                  "Maximum Similarity of picture (i, j) (0, 1): 103\n",
                  "Maximum Similarity of picture (i, j) (1, 0): 112\n",
                  "Maximum Similarity of picture (i, j) (1, 1): 121\n"
               ]
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 2000x1000 with 4 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "from animation.util import backtransform_weights, reconstruct_image\n",
            "from networks.mlp_models import MLP3D\n",
            "\n",
            "ij_len = 2\n",
            "# Plotting the tensors as heatmaps in grayscale\n",
            "fig, axes = plt.subplots(ij_len, ij_len, figsize=(20, 10))\n",
            "\n",
            "SOS = torch.Tensor([[0]]).long().to(device)\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": 5,\n",
            "}\n",
            "\n",
            "for i in range(ij_len):\n",
            "    for j in range(ij_len):\n",
            "\n",
            "        model.eval()\n",
            "        novel_tokens = model.generate(SOS, dataset[0][0].shape[0], temperature=1.0, top_k=None)[:, 1:].unsqueeze(-1).to(\"cpu\")\n",
            "\n",
            "        max_similarity = 0\n",
            "\n",
            "        for data in dataset:\n",
            "            similarity = (data[0].to(device)==novel_tokens.squeeze(-1).squeeze(0).to(device)).int().sum()\n",
            "            if similarity > max_similarity:\n",
            "                max_similarity = similarity\n",
            "\n",
            "        print(f\"Maximum Similarity of picture (i, j) {(i, j)}: {max_similarity}\")\n",
            "\n",
            "        novel_weights= vq.get_codes_from_indices((novel_tokens-1))\n",
            "\n",
            "        dataset_no_transform = MnistNeFDataset(os.path.join(data_root, \"datasets\", \"mnist-nerfs\"), **kwargs)\n",
            "        original_dict = dataset_no_transform[0][0]\n",
            "\n",
            "        reconstructed_dict = backtransform_weights(novel_weights, original_dict[\"state_dict\"])\n",
            "\n",
            "        mlp3d = MLP3D(**original_dict[\"model_config\"])\n",
            "        mlp3d.load_state_dict(reconstructed_dict)\n",
            "        reconstructed_tensor = reconstruct_image(mlp3d)\n",
            "\n",
            "        axes[i][j].imshow(reconstructed_tensor, cmap='gray', aspect='auto')\n",
            "\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.14"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
