{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "import os\n",
    "from data.neural_field_datasets_shapenet import ShapeNetDataset, FlattenTransform3D, TokenTransform3D, ModelTransform3DFromTokens, ModelTransform3D\n",
    "from training import training_nano_gpt\n",
    "from utils.visualization3d import visualize_model3d, model_to_mesh\n",
    "\n",
    "from networks.nano_gpt import GPTConfig\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "\"type\": \"pretrained\",\n",
    "\"fixed_label\": None,\n",
    "}\n",
    "\n",
    "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(dir_path, \"adl4cv\")\n",
    "\n",
    "# load used vector quantizer\n",
    "vq_dicts = torch.load(os.path.join(data_root, \"models\", \"vq_search_results\", \"vq_model_dim_1_vocab_255_batch_size_32768_threshold_ema_dead_code_0_kmean_iters_0.pth\"))\n",
    "vq = VectorQuantize(**vq_dicts[\"vq_config\"])\n",
    "vq.load_state_dict(vq_dicts[\"state_dict\"])\n",
    "vq.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShapeNetDataset(os.path.join(data_root, \"datasets\", \"plane_mlp_weights\"), transform=TokenTransform3D(vq))\n",
    "dataset_flatten = ShapeNetDataset(os.path.join(data_root, \"datasets\", \"plane_mlp_weights\"), transform=FlattenTransform3D())\n",
    "dataset_model = ShapeNetDataset(os.path.join(data_root, \"datasets\", \"plane_mlp_weights\"), transform=ModelTransform3D())\n",
    "backtransform = ModelTransform3DFromTokens(vq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the files in models/vq_search_results using glob\n",
    "\n",
    "\n",
    "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(dir_path, \"adl4cv\")\n",
    "\n",
    "test_idx = 14\n",
    "\n",
    "for root, dirs, files in os.walk('models/vq_search_results'):\n",
    "    losses = []\n",
    "    for file in files:\n",
    "        \n",
    "        if file.endswith('.pth'):\n",
    "            vq_dict = torch.load(os.path.join(data_root, \"models\", \"vq_search_results\", file))\n",
    "            loss = vq_dict[\"loss\"][-1]\n",
    "            losses.append(loss)\n",
    "\n",
    "    # find top n min loss idx\n",
    "    n = 5\n",
    "    min_loss_idx = sorted(range(len(losses)), key=lambda i: losses[i])[:n]\n",
    "    for idx in min_loss_idx:\n",
    "        print(f\"Found best configuration {files[idx]} with loss {losses[idx]} from {len(losses)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh, sdf = model_to_mesh(dataset_model[0][0].cuda())\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh, sdf = model_to_mesh(backtransform(dataset[0][0].cuda(), None)[0].cuda())\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Training\n",
    "config = training_nano_gpt.Config()\n",
    "config.learning_rate=3e-3\n",
    "config.max_iters = 30000\n",
    "config.weight_decay=0.00\n",
    "config.decay_lr=True\n",
    "config.lr_decay_iters=config.max_iters\n",
    "config.warmup_iters=0.05*config.max_iters\n",
    "config.batch_size = 8\n",
    "config.gradient_accumulation_steps = 1\n",
    "config.init_from = \"scratch\"\n",
    "config.out_dir =\"models/shapenet_token_transformer\"\n",
    "config.detailed_folder = \"training_sample_5\"\n",
    "\n",
    "config.wandb_project = \"shapenet_token_transformer\"\n",
    "\n",
    "config.eval_interval = 250\n",
    "config.metric_interval = 250\n",
    "\n",
    "max_len = dataset[0][0].size(0)\n",
    "model_config = GPTConfig(n_embd=64, block_size=512, n_head=8, n_layer=8, vocab_size=vq_dicts[\"vq_config\"][\"codebook_size\"] + 1, dropout=0.0, max_len= max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_size = vq_dicts[\"vq_config\"][\"codebook_size\"]\n",
    "token_dict = {\n",
    "    \"SOS\": cb_size,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to put?\n",
    "# Maybe adjust dataset to be able to work with splitting data and then rewrite TokenTransform \n",
    "# to do the job combined with pytorch dataloader (get_batch == __call__ of Dataloader)\n",
    "\n",
    "def create_split_indices(n, train_ratio=0.9):\n",
    "    # Generate a random permutation of indices from 0 to n-1\n",
    "    shuffled_indices = torch.randperm(n)\n",
    "    # Determine the cut-off for training data\n",
    "    train_size = int(train_ratio * n)\n",
    "    # Split indices into training and validation sets\n",
    "    train_indices = shuffled_indices[:train_size]\n",
    "    val_indices = shuffled_indices[train_size:]\n",
    "    return train_indices, val_indices\n",
    "\n",
    "train_indices, val_indices = create_split_indices(len(dataset))\n",
    "\n",
    "def get_batch_lambda(config, dataset, model_config, split):\n",
    "    batch_size = config.batch_size\n",
    "    \n",
    "\n",
    "    # Select indices based on the split\n",
    "    if split == 'train':\n",
    "        # Randomly select batch_size indices from the train_indices\n",
    "        indices = train_indices[torch.randint(0, len(train_indices), (batch_size,))]\n",
    "    elif split == 'val':\n",
    "        # Randomly select batch_size indices from the val_indices\n",
    "        indices = val_indices[torch.randint(0, len(val_indices), (batch_size,))]\n",
    "    \n",
    "    \n",
    "    # Initialize lists to hold the sequences and labels\n",
    "    samples = []\n",
    "    labels = []\n",
    "\n",
    "    # Collect samples and labels\n",
    "    for idx in indices:\n",
    "        sample, label = dataset[idx]\n",
    "        start_tokens = torch.Tensor([token_dict[\"SOS\"]]).long().to(sample)  # Start of sequence token\n",
    "        sample = torch.cat((start_tokens, sample), dim=0)\n",
    "        #start_tokens = torch.Tensor([0]).long()  # Start of sequence token\n",
    "        #sample = torch.cat((start_tokens, sample + 1), dim=0)\n",
    "        samples.append(sample)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Prepare the sequences for model input\n",
    "    max_len = samples[0].size(0)\n",
    "    x = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
    "    y = torch.zeros((batch_size, max_len - 1), dtype=torch.long)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        end_index = sample.size(0) - 1\n",
    "        x[i, :end_index] = sample[:-1]  # Exclude the last token for x\n",
    "        y[i, :end_index] = sample[1:]   # Exclude the first token for y\n",
    "\n",
    "    idx = torch.randint(0, max_len - 1 - model_config.block_size, (batch_size,))\n",
    "    x_cutted = torch.zeros((batch_size, model_config.block_size), dtype=torch.long)\n",
    "    y_cutted = torch.zeros((batch_size, model_config.block_size), dtype=torch.long)\n",
    "\n",
    "    for i, offset in enumerate(idx):\n",
    "        x_cutted[i, :] = x[i, offset:offset + model_config.block_size]\n",
    "        y_cutted[i, :] = y[i, offset:offset + model_config.block_size]\n",
    "\n",
    "    # x and y have to be\n",
    "    x_cutted = x_cutted.to(config.device)\n",
    "    y_cutted = y_cutted.to(config.device)\n",
    "    idx = idx.to(config.device)\n",
    "\n",
    "    return x_cutted, y_cutted, idx\n",
    "\n",
    "create_get_batch = lambda config, dataset, model_config: lambda split: get_batch_lambda(config, dataset, model_config, split)\n",
    "get_batch = create_get_batch(config, dataset, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepeare model parameters and train\n",
    "import wandb\n",
    "trained_model = training_nano_gpt.train(get_batch, config, model_config, vq, vq_dicts[\"vq_config\"], token_dict=token_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from networks.nano_gpt import GPT\n",
    "from utils import get_default_device\n",
    "\n",
    "model_dict = torch.load(\"./models/shapenet_token_transformer/ckpt.pt\")\n",
    "# Configuration\n",
    "print(model_dict.keys())\n",
    "idx = 3\n",
    "\n",
    "device = get_default_device()\n",
    "model = GPT(model_dict[\"c\"])#model_dict\n",
    "model.to(device=device)\n",
    "model.load_state_dict(model_dict[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "vq = VectorQuantize(**model_dict[\"vq_config\"])\n",
    "vq.load_state_dict(model_dict[\"vq_state_dict\"])\n",
    "vq.to(device=device)\n",
    "vq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# time the generation\n",
    "import time\n",
    "start = time.time()\n",
    "novel_tokens = model.generate(torch.Tensor([[token_dict[\"SOS\"]]]).long().to(device=\"cuda\"), max_len, temperature=20, top_k=1)\n",
    "print(f\"Time for generation: {time.time() - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = novel_tokens.squeeze(0)[1:]\n",
    "mlp_model, label = ModelTransform3DFromTokens(vq)(tokens.detach(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh, sdf = model_to_mesh(mlp_model.cuda())\n",
    "mesh.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
