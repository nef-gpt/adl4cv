{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Enable autoreload of module\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# log python version\n",
            "import sys\n",
            "print(sys.version)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from training import training_regression_transformer\n",
            "from networks.regression_transformer import RegressionTransformerConfig, RegressionTransformer\n",
            "\n",
            "from data.nef_mnist_dataset import MnistNeFDataset, FlattenTransform, MinMaxTransform\n",
            "\n",
            "import os\n",
            "import torch\n",
            "import torchinfo"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "torch.cuda.is_available()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Dataloading\n",
            "dir_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
            "data_root_ours = os.path.join(dir_path, \"adl4cv\", \"datasets\", \"mnist-nerfs\")\n",
            "\n",
            "class FlattenMinMaxTransform(torch.nn.Module):\n",
            "  def __init__(self, min_max: tuple = None):\n",
            "    super().__init__()\n",
            "    self.flatten = FlattenTransform()\n",
            "    if min_max:\n",
            "      self.minmax = MinMaxTransform(*min_max)\n",
            "    else:\n",
            "      self.minmax = MinMaxTransform()\n",
            "\n",
            "  def forward(self, x, y):\n",
            "    x, _ = self.flatten(x, y)\n",
            "    x, _ = self.minmax(x, y)\n",
            "    return x, y\n",
            "\n",
            "\n",
            "kwargs = {\n",
            "\"type\": \"pretrained\",\n",
            "\"fixed_label\": 5,\n",
            "}\n",
            "\n",
            "\n",
            "dataset_wo_min_max = MnistNeFDataset(data_root_ours, transform=FlattenTransform(), **kwargs)\n",
            "min_ours, max_ours = dataset_wo_min_max.min_max()\n",
            "dataset = MnistNeFDataset(data_root_ours, transform=FlattenMinMaxTransform((min_ours, max_ours)), **kwargs)\n",
            "dataset_no_transform = MnistNeFDataset(data_root_ours, **kwargs)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Config Training\n",
            "config = training_regression_transformer.Config()\n",
            "config.learning_rate=5e-4\n",
            "config.max_iters = 14000\n",
            "config.weight_decay=0\n",
            "config.decay_lr=True\n",
            "config.lr_decay_iters=14000\n",
            "config.warmup_iters=0.1*config.max_iters\n",
            "config.batch_size = 1\n",
            "config.detailed_folder = \"training_sample_5\"\n",
            "\n",
            "# Config Transforemer\n",
            "model_config = RegressionTransformerConfig(n_embd=32, block_size=len(dataset[0][0]) - 1, n_head=8, n_layer=16)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# take first n samples that have label == 1 (where label is second entry of dataset object)\n",
            "n = 5\n",
            "samples = [(i, dataset[i][0]) for i in range(len(dataset)) if dataset[i][1] == 5][:n]\n",
            "\n",
            "\n",
            "def get_batch(split: str):\n",
            "    # let's get a batch with the single element\n",
            "    # y should be the same shifted by 1\n",
            "    ix = torch.zeros(config.batch_size, dtype=torch.int)\n",
            "    #torch.randint(torch.numel(flattened) - model_config.block_size, (config.batch_size,))\n",
            "\n",
            "    # randomly select a sample (0...n-1)\n",
            "    split_start = 0 if split == \"train\" else int(0.8 * n)\n",
            "    split_end = int(0.8 * n) if split == \"train\" else n\n",
            "\n",
            "    sample = samples[torch.randint(split_start, split_end, (1,))][1]\n",
            "\n",
            "    x = torch.stack(\n",
            "        [sample[i : i + model_config.block_size] for i in ix]\n",
            "    )\n",
            "    y = torch.stack(\n",
            "        [sample[i + 1 : i + 1 + model_config.block_size] for i in ix]\n",
            "    )\n",
            "\n",
            "    # x and y have to be (1, *, 1)\n",
            "    x = x.unsqueeze(-1).to(config.device)\n",
            "    y = y.unsqueeze(-1).to(config.device)\n",
            "    return x, y"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Prepeare model parameters and train\n",
            "training_regression_transformer.train(get_batch, config, model_config)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.3"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
